{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering for Repository Summarization\n",
    "\n",
    "This notebook is designed to help you experiment with different prompts to generate high-quality summaries for government repositories. \n",
    "\n",
    "You can:\n",
    "1.  Load your repository data from a CSV file.\n",
    "2.  Take a random sample of repositories to test.\n",
    "3.  Define and test multiple summarization prompts.\n",
    "4.  Generate summaries using a language model.\n",
    "5.  Compare the results in a clear, side-by-side format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_FILE_PATH = '/home/theo/projects/iai/data/tmprunid/classified_gov_repositories_batch.csv'\n",
    "SAMPLE_SIZE = 10  # Number of random repositories to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Sample Data\n",
    "\n",
    "We'll load the data from the CSV file and select a random sample of repositories to work with. This keeps the process fast and manageable for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 500 repositories and sampled 20 of them.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>readme</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>rdl-standard</td>\n",
       "      <td># Risk Data Library Standard\\n\\nThe Risk Data Library Standard is a data model for describing Hazard, Exposure, \\nVulnerability and Loss data.\\n\\nThe model describes the common core metadata that applies to all risk datasets, \\nas well as standardised metadata that applies to Hazard, Exposure, Vulnerability and Loss \\ndata.\\n\\nThis repository is used to coordinate the development of this data model. It will \\nbe used to:\\n\\n* publish working and released drafts of the data model specifications\\n* coordinate collaboration and discussion around the iterative development of those specifications\\n* provide an overview of the current status and roadmap\\n\\n## Intended audience\\n\\nThe repository is intended to support the work of those developing and contributing to the \\nRisk Data Library specifications.\\n\\nThis repository is intended to:\\n\\n* support comments or feedback on the current specifications\\n* propose and discuss changes, e.g. in the form of revised wording or additions to the model\\n* answer questions about the governance and evolution of the standard\\n\\nOther more useful resources exist if you have general questions about the scope and goals \\nof [the Risk Data Library project](http://riskdatalibrary.org/), or are looking for a more [high-level introduction to \\nthe standard and its key concepts](https://docs.riskdatalibrary.org/).\\n\\n## How to contribute\\n\\nThe [Contributors guide](CONTRIBUTING.md) covers the different ways in which you can contribute to this project to \\nsupport the development and adoption of the Risk Data Library Standard.\\n\\n## Project governance\\n\\nRead [the project governance documentation](GOVERNANCE.md) for more detail about our approach to making decisions and \\nagreeing changes to the standard.\\n\\n## Licence\\n\\nThe published specifications and all working documents in this repository are published under \\na [Creative Commons Attribution-ShareAlike 4.0 (CC-BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode) licence.\\n\\nVisit the Creative Commons website for [official translations of the licence text](https://creativecommons.org/licenses/by-sa/4.0/legalcode#languages).\\n</td>\n",
       "      <td>The Risk Data Library Standard (RDLS) is an open data standard to make it easier to work with disaster and climate risk data. It provides a common description of the data used and produced in risk assessments, including hazard, exposure, vulnerability, and modelled loss, or impact, data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>GeoNature-mobile-webapi</td>\n",
       "      <td># GeoNature-mobile-webapi\\n\\nGeoNature est une application de saisie et de synth√®se des observations faune et flore : https://github.com/PnEcrins/GeoNature\\n\\nPour pouvoir importer les donn√©es saisies avec [Geonature-mobile](https://github.com/PnEcrins/GeoNature-mobile) dans la BDD PostgreSQL de GeoNature, cette web-API doit √™tre install√©e sur le serveur.\\n\\nLa synchronisation de ces donn√©es peut √™tre faite par le r√©seau (wifi ou 3G) ou en connectant le mobile en USB √† un PC connect√© √† internet. Dans ce cas, une application de synchronisation des donn√©es soit √™tre install√©e sur le PC : https://github.com/PnEcrins/GeoNature-mobile-sync \\n\\n![GeoNature schema general](https://github.com/PnEcrins/GeoNature/raw/master/docs/images/schema-geonature-environnement.jpg)\\n\\n## License\\n&amp;copy; Makina Corpus / Parc national des Ecrins 2012 - 2017\\n</td>\n",
       "      <td>WebAPI (cot√© serveur) de synchronisation des donn√©es produites par GeoNature-mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>portal-brasil</td>\n",
       "      <td>&lt;div align=\"center\"&gt;&lt;img alt=\"logo\" src=\"https://raw.githubusercontent.com/plonegovbr/plonegovbr.portal/main/docs/logo.png\" width=\"150\" /&gt;&lt;/div&gt;\\n\\n&lt;h1 align=\"center\"&gt;PortalBrasil&lt;/h1&gt;\\n\\nProjeto de desenvolvimento do Portal Brasil\\n\\n## Instala√ß√£o\\n\\nClone este reposit√≥rio\\n\\n```bash\\ngit clone git@github.com:plonegovbr/portal-brasil.git\\n```\\n\\nInstale as depend√™ncias de backend\\n\\n```bash\\nmake install-backend\\n```\\n\\nInstale as depend√™ncias de frontend\\n\\n```bash\\nmake install-frontend\\n```\\n\\n## Inicie os servidores\\n\\nInicie o servidor de backend\\n\\n```bash\\nmake start-backend\\n```\\nEm outro terminal, inicie o servidor de frontend:\\n\\n```bash\\nmake start-frontend\\n```\\n\\n## Pacotes em desenvolvimento\\n\\n### Backend\\n\\nEdite o arquivo `backend/mx.ini` e adicione / edite os pacotes e rode `make install-backend` novamente.\\n\\n### Frontend\\n\\nEdite o arquivo `frontend/mrs.developer.json` e adicione / edite os pacotes e rode `make install-frontend` novamente.\\n</td>\n",
       "      <td>Ambiente de desenvolvimento do PortalBrasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Alouette_ISIS_extract</td>\n",
       "      <td>\\n# Alouette-1, ISIS - 1 and ISIS -2 - Ionogram Data Extraction - Data from Canada's First Satellites Over 60 Years In the Making\\n\\n&gt; In this project, the film rolls from the Alouette and ISIS satellites were scanned, digitized, and made accessible to the public. The primary aim was to establish a centralized data repository, facilitating access for researchers to utilize both the data and metadata derived from the satellites for future research.\\n\\nAlouette -1 was the first topside ionospheric satellite and the first Canadian satellite launched in 1962 in collaboration with the United States through NASA. Alouette ‚Äì 1 was known for its swept frequency topside sounder experiment with the goal to investigate the geographic and diurnal variation of the topside ionosphere at altitudes up to 1000 km. One of the most important scientific results from Alouette-1 was that it provided the first global picture of electron-density distribution in the topside ionosphere. With the success of Alouette -1, Canada and the United States formally agreed on December 23rd, 1963, to extend their collaboration to a program called International Satellites for Ionospheric studies (ISIS). As part of this program, Canada designed and built an additional family of ionospheric satellites: Alouette ‚Äì 2, ISIS -1 and ISIS ‚Äì 2. The ISIS - 1 and ISIS - 2 satellites had a more complex navigational systems and larger data collection capabilities than Alouette- 1 and 2 satellites. For instance, ISIS ‚Äì 1 was the first in the series to contain a swept and fixed frequency sounder technique combined with a complete set of direct measurements.\\n\\nThe output from the topside sounders were a video signal that contained the ionospheric echo pulses, but also pulses that depicted frequency markers and when a new frame started. A system was built to read the 7-track reel-to-reel magnetic tapes displayed on a cathode ray tube in ‚ÄòB-scan‚Äô form. This product was called an ionogram, which depicted the reflections of radio waves emitted from the satellite off the top side of the ionosphere, across a range of frequencies. The scanning of the ionograms as the first step of the historical data restoration of the Alouette and ISIS satellites began in 2017. The processing of the Alouette \\n and ISIS data was concluded in 2023 and 2024 respectively. \\n\\n&gt; Canadian Space Agency has created a centralized repository, facilitating easy access for researchers to utilize both the data and metadata derived from the Alouette and ISIS satellites. This includes but is not limited to open-source code on the processing of the data, raw images, data dictionaries, detailed methodology and a micro application that provides users the ability to select, download and visualize Alouette and ISIS data.\\n\\n\\n## How to Get Started\\n**To learn how to access, work and re-process the data, read:**\\n\\n- [**Alouette-1 ‚Äì Ionogram Data Extraction Methodology**](https://github.com/asc-csa/Alouette_extract/blob/working/documentation/Alouette-1%20-</td>\n",
       "      <td>üõ∞Ô∏è Ce code sert √† extraire les donn√©es et les m√©tadonn√©es des ionogrammes num√©ris√©s des satellites Alouette et ISIS |  üõ∞Ô∏è This code is an effort to extract data and metadata from the scanned ionogram images from the Alouette and ISIS satellites.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>quadratic-voting-frontend</td>\n",
       "      <td># Quadratic Voting Frontend\\n\\nÊ≠§ËôïÁÇ∫[2019Á∏ΩÁµ±ÁõÉÈªëÂÆ¢Êùæ](https://presidential-hackathon.taiwan.gov.tw/)Âπ≥ÊñπÊäïÁ•®Ê≥ï‰πãÂâçÁ´ØÁ®ãÂºèÁ¢ºÔºå‰æõÂ§ßÁúæÂèÉËÄÉÂà©Áî®„ÄÇ\\n\\nÊ≥®ÊÑèÔºöÊ≠§Á®ãÂºèÁ¢º‰∏çÂê´ÂæåÁ´ØÁ®ãÂºèÁ¢º\\n\\nThis is the frontend code of [Taiwan Presidential Hackathon 2019](https://presidential-hackathon.taiwan.gov.tw/en/Default.aspx) quadratic voting page, and open under MIT License for public use.\\n\\nNotice: This code did not include backend.\\n\\n## ÊäïÁ•®ÁµêÊûú\\n\\nÊ≠§ËôïË≥áÊñôÁÇ∫ÊäïÁ•®Âæå‰πãË≥áÊñôÔºå‰æõÂ§ßÁúæÁ†îÁ©∂Âà©Áî®„ÄÇ\\n\\n### ÊèêÊ°àË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[Proposal.json](data/Proposal.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- ServiceAgencies: ÂúòÈ´î/Ê©üÊßãÂêçÁ®±\\n- TeamName: ÈöäÂêç\\n- ProposalTitle: È°åÁõÆ\\n\\n### ÊäïÁ•®ÁµêÊûúË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[ProposalPolls.json](data/ProposalPolls.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- Count: ÂæóÁ•®Êï∏\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n### ‰ΩøÁî®ËÄÖË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[User.json](data/User.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n### ‰ΩøÁî®ËÄÖÁ¥ÄÈåÑ\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[UserAction.json](data/UserAction.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- ActionID: Â∫èËôü\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- Sequence: Ë©≤Ê¨°ÊäïÁ•®Èö®Ê©üÊéíÂ∫èÂá∫ÁèæÂú®Á¨¨N‰Ωç\\n- Action: ÊäïÁ•®ÊàñÂõûÊî∂\\n  - Add: ÊäïÁ•®\\n  - Sub: Êî∂Âõû‰∏ÄÁ•®\\n- VoteCount: ÊäïÁ•®ÊàñÊî∂ÂõûÂæåÂâ©NÁ•®\\n- SessionID: SessionË≠òÂà•Á¢º\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n# Author\\n\\nÈô≥‰∏ñÁ••\\n\\n# License\\n\\n[MIT](License)\\n\\n</td>\n",
       "      <td>No description</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  \\\n",
       "83                rdl-standard   \n",
       "235    GeoNature-mobile-webapi   \n",
       "207              portal-brasil   \n",
       "168      Alouette_ISIS_extract   \n",
       "135  quadratic-voting-frontend   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       readme  \\\n",
       "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             # Risk Data Library Standard\\n\\nThe Risk Data Library Standard is a data model for describing Hazard, Exposure, \\nVulnerability and Loss data.\\n\\nThe model describes the common core metadata that applies to all risk datasets, \\nas well as standardised metadata that applies to Hazard, Exposure, Vulnerability and Loss \\ndata.\\n\\nThis repository is used to coordinate the development of this data model. It will \\nbe used to:\\n\\n* publish working and released drafts of the data model specifications\\n* coordinate collaboration and discussion around the iterative development of those specifications\\n* provide an overview of the current status and roadmap\\n\\n## Intended audience\\n\\nThe repository is intended to support the work of those developing and contributing to the \\nRisk Data Library specifications.\\n\\nThis repository is intended to:\\n\\n* support comments or feedback on the current specifications\\n* propose and discuss changes, e.g. in the form of revised wording or additions to the model\\n* answer questions about the governance and evolution of the standard\\n\\nOther more useful resources exist if you have general questions about the scope and goals \\nof [the Risk Data Library project](http://riskdatalibrary.org/), or are looking for a more [high-level introduction to \\nthe standard and its key concepts](https://docs.riskdatalibrary.org/).\\n\\n## How to contribute\\n\\nThe [Contributors guide](CONTRIBUTING.md) covers the different ways in which you can contribute to this project to \\nsupport the development and adoption of the Risk Data Library Standard.\\n\\n## Project governance\\n\\nRead [the project governance documentation](GOVERNANCE.md) for more detail about our approach to making decisions and \\nagreeing changes to the standard.\\n\\n## Licence\\n\\nThe published specifications and all working documents in this repository are published under \\na [Creative Commons Attribution-ShareAlike 4.0 (CC-BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode) licence.\\n\\nVisit the Creative Commons website for [official translations of the licence text](https://creativecommons.org/licenses/by-sa/4.0/legalcode#languages).\\n   \n",
       "235                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          # GeoNature-mobile-webapi\\n\\nGeoNature est une application de saisie et de synth√®se des observations faune et flore : https://github.com/PnEcrins/GeoNature\\n\\nPour pouvoir importer les donn√©es saisies avec [Geonature-mobile](https://github.com/PnEcrins/GeoNature-mobile) dans la BDD PostgreSQL de GeoNature, cette web-API doit √™tre install√©e sur le serveur.\\n\\nLa synchronisation de ces donn√©es peut √™tre faite par le r√©seau (wifi ou 3G) ou en connectant le mobile en USB √† un PC connect√© √† internet. Dans ce cas, une application de synchronisation des donn√©es soit √™tre install√©e sur le PC : https://github.com/PnEcrins/GeoNature-mobile-sync \\n\\n![GeoNature schema general](https://github.com/PnEcrins/GeoNature/raw/master/docs/images/schema-geonature-environnement.jpg)\\n\\n## License\\n&copy; Makina Corpus / Parc national des Ecrins 2012 - 2017\\n   \n",
       "207                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <div align=\"center\"><img alt=\"logo\" src=\"https://raw.githubusercontent.com/plonegovbr/plonegovbr.portal/main/docs/logo.png\" width=\"150\" /></div>\\n\\n<h1 align=\"center\">PortalBrasil</h1>\\n\\nProjeto de desenvolvimento do Portal Brasil\\n\\n## Instala√ß√£o\\n\\nClone este reposit√≥rio\\n\\n```bash\\ngit clone git@github.com:plonegovbr/portal-brasil.git\\n```\\n\\nInstale as depend√™ncias de backend\\n\\n```bash\\nmake install-backend\\n```\\n\\nInstale as depend√™ncias de frontend\\n\\n```bash\\nmake install-frontend\\n```\\n\\n## Inicie os servidores\\n\\nInicie o servidor de backend\\n\\n```bash\\nmake start-backend\\n```\\nEm outro terminal, inicie o servidor de frontend:\\n\\n```bash\\nmake start-frontend\\n```\\n\\n## Pacotes em desenvolvimento\\n\\n### Backend\\n\\nEdite o arquivo `backend/mx.ini` e adicione / edite os pacotes e rode `make install-backend` novamente.\\n\\n### Frontend\\n\\nEdite o arquivo `frontend/mrs.developer.json` e adicione / edite os pacotes e rode `make install-frontend` novamente.\\n   \n",
       "168  \\n# Alouette-1, ISIS - 1 and ISIS -2 - Ionogram Data Extraction - Data from Canada's First Satellites Over 60 Years In the Making\\n\\n> In this project, the film rolls from the Alouette and ISIS satellites were scanned, digitized, and made accessible to the public. The primary aim was to establish a centralized data repository, facilitating access for researchers to utilize both the data and metadata derived from the satellites for future research.\\n\\nAlouette -1 was the first topside ionospheric satellite and the first Canadian satellite launched in 1962 in collaboration with the United States through NASA. Alouette ‚Äì 1 was known for its swept frequency topside sounder experiment with the goal to investigate the geographic and diurnal variation of the topside ionosphere at altitudes up to 1000 km. One of the most important scientific results from Alouette-1 was that it provided the first global picture of electron-density distribution in the topside ionosphere. With the success of Alouette -1, Canada and the United States formally agreed on December 23rd, 1963, to extend their collaboration to a program called International Satellites for Ionospheric studies (ISIS). As part of this program, Canada designed and built an additional family of ionospheric satellites: Alouette ‚Äì 2, ISIS -1 and ISIS ‚Äì 2. The ISIS - 1 and ISIS - 2 satellites had a more complex navigational systems and larger data collection capabilities than Alouette- 1 and 2 satellites. For instance, ISIS ‚Äì 1 was the first in the series to contain a swept and fixed frequency sounder technique combined with a complete set of direct measurements.\\n\\nThe output from the topside sounders were a video signal that contained the ionospheric echo pulses, but also pulses that depicted frequency markers and when a new frame started. A system was built to read the 7-track reel-to-reel magnetic tapes displayed on a cathode ray tube in ‚ÄòB-scan‚Äô form. This product was called an ionogram, which depicted the reflections of radio waves emitted from the satellite off the top side of the ionosphere, across a range of frequencies. The scanning of the ionograms as the first step of the historical data restoration of the Alouette and ISIS satellites began in 2017. The processing of the Alouette \\n and ISIS data was concluded in 2023 and 2024 respectively. \\n\\n> Canadian Space Agency has created a centralized repository, facilitating easy access for researchers to utilize both the data and metadata derived from the Alouette and ISIS satellites. This includes but is not limited to open-source code on the processing of the data, raw images, data dictionaries, detailed methodology and a micro application that provides users the ability to select, download and visualize Alouette and ISIS data.\\n\\n\\n## How to Get Started\\n**To learn how to access, work and re-process the data, read:**\\n\\n- [**Alouette-1 ‚Äì Ionogram Data Extraction Methodology**](https://github.com/asc-csa/Alouette_extract/blob/working/documentation/Alouette-1%20-   \n",
       "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               # Quadratic Voting Frontend\\n\\nÊ≠§ËôïÁÇ∫[2019Á∏ΩÁµ±ÁõÉÈªëÂÆ¢Êùæ](https://presidential-hackathon.taiwan.gov.tw/)Âπ≥ÊñπÊäïÁ•®Ê≥ï‰πãÂâçÁ´ØÁ®ãÂºèÁ¢ºÔºå‰æõÂ§ßÁúæÂèÉËÄÉÂà©Áî®„ÄÇ\\n\\nÊ≥®ÊÑèÔºöÊ≠§Á®ãÂºèÁ¢º‰∏çÂê´ÂæåÁ´ØÁ®ãÂºèÁ¢º\\n\\nThis is the frontend code of [Taiwan Presidential Hackathon 2019](https://presidential-hackathon.taiwan.gov.tw/en/Default.aspx) quadratic voting page, and open under MIT License for public use.\\n\\nNotice: This code did not include backend.\\n\\n## ÊäïÁ•®ÁµêÊûú\\n\\nÊ≠§ËôïË≥áÊñôÁÇ∫ÊäïÁ•®Âæå‰πãË≥áÊñôÔºå‰æõÂ§ßÁúæÁ†îÁ©∂Âà©Áî®„ÄÇ\\n\\n### ÊèêÊ°àË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[Proposal.json](data/Proposal.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- ServiceAgencies: ÂúòÈ´î/Ê©üÊßãÂêçÁ®±\\n- TeamName: ÈöäÂêç\\n- ProposalTitle: È°åÁõÆ\\n\\n### ÊäïÁ•®ÁµêÊûúË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[ProposalPolls.json](data/ProposalPolls.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- Count: ÂæóÁ•®Êï∏\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n### ‰ΩøÁî®ËÄÖË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[User.json](data/User.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n### ‰ΩøÁî®ËÄÖÁ¥ÄÈåÑ\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[UserAction.json](data/UserAction.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- ActionID: Â∫èËôü\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- Sequence: Ë©≤Ê¨°ÊäïÁ•®Èö®Ê©üÊéíÂ∫èÂá∫ÁèæÂú®Á¨¨N‰Ωç\\n- Action: ÊäïÁ•®ÊàñÂõûÊî∂\\n  - Add: ÊäïÁ•®\\n  - Sub: Êî∂Âõû‰∏ÄÁ•®\\n- VoteCount: ÊäïÁ•®ÊàñÊî∂ÂõûÂæåÂâ©NÁ•®\\n- SessionID: SessionË≠òÂà•Á¢º\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n# Author\\n\\nÈô≥‰∏ñÁ••\\n\\n# License\\n\\n[MIT](License)\\n\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                          description  \n",
       "83   The Risk Data Library Standard (RDLS) is an open data standard to make it easier to work with disaster and climate risk data. It provides a common description of the data used and produced in risk assessments, including hazard, exposure, vulnerability, and modelled loss, or impact, data.  \n",
       "235                                                                                                                                                                                                               WebAPI (cot√© serveur) de synchronisation des donn√©es produites par GeoNature-mobile  \n",
       "207                                                                                                                                                                                                                                                       Ambiente de desenvolvimento do PortalBrasil  \n",
       "168                                             üõ∞Ô∏è Ce code sert √† extraire les donn√©es et les m√©tadonn√©es des ionogrammes num√©ris√©s des satellites Alouette et ISIS |  üõ∞Ô∏è This code is an effort to extract data and metadata from the scanned ionogram images from the Alouette and ISIS satellites.  \n",
       "135                                                                                                                                                                                                                                                                                    No description  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(CSV_FILE_PATH)\n",
    "    # Ensure the sample size is not larger than the number of rows in the dataframe\n",
    "    sample_size = min(SAMPLE_SIZE, len(df))\n",
    "    if len(df) > sample_size:\n",
    "        sample_df = df.sample(n=sample_size, random_state=62) # random_state for reproducibility\n",
    "    else:\n",
    "        sample_df = df\n",
    "    print(f\"Successfully loaded {len(df)} repositories and sampled {len(sample_df)} of them.\")\n",
    "    display(sample_df[['name', 'readme', 'description']].head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {CSV_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define LLM Utility\n",
    "\n",
    "This is where you'll integrate your language model. The function `get_summary` is a placeholder. **You should replace its content with the logic from your `llm_utils.py` file.**\n",
    "\n",
    "The function should accept a `prompt` and the `text_to_summarize` and return the generated summary as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Prompts for Engineering\n",
    "\n",
    "Here you can define all the different prompts you want to test. I've added a few examples to get you started, focusing on different aspects like tone, format, and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reportify is a Python library that simplifies data reporting.  It transforms JSON or dictionary data into aesthetically pleasing PDF or HTML reports.  The library offers customizable templates and integrates easily with pandas DataFrames.  Users can generate formatted reports from complex data structures.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install necessary libraries if you haven't already\n",
    "# !pip install langchain-google-genai langchain python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from your .env file\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "# Securely get the API key from the environment.\n",
    "# This prevents the key from being stored in the notebook's code or output.\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Make sure it's in your .env file.\")\n",
    "\n",
    "# Define the model name (as seen in llm_utils.py)\n",
    "LLM_MODEL_NAME = \"gemini-1.5-flash\"\n",
    "\n",
    "# Initialize the Google Generative AI model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=LLM_MODEL_NAME,\n",
    "    google_api_key=google_api_key,\n",
    ")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# You can replace these variables with your own data\n",
    "repo_description = \"A Python library for parsing and generating beautiful, human-readable reports from complex data structures.\"\n",
    "repo_readme = \"\"\"\n",
    "# Reportify v1.2\n",
    "\n",
    "Reportify is a tool designed to make data reporting simple. It takes JSON or dictionary data and outputs clean, formatted reports in PDF or HTML.\n",
    "\n",
    "## Features\n",
    "- Multiple output formats (PDF, HTML)\n",
    "- Customizable templates\n",
    "- Easy integration with pandas DataFrames\n",
    "\n",
    "## Getting Started\n",
    "`pip install reportify`\n",
    "\"\"\"\n",
    "\n",
    "def get_summary(prompt: str, repo_description: str, repo_readme:str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a summary for the given text using a specified prompt.\n",
    "    This is a placeholder and should be replaced with your actual LLM implementation.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt to use for the summarization task.\n",
    "        repo_description: ...\n",
    "        repo_readme: ...\n",
    "\n",
    "    Returns:\n",
    "        The generated summary as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # This is the prompt template copied directly from llm_utils.py\n",
    "    summary_prompt = PromptTemplate(\n",
    "        input_variables=[\"description\", \"readme\"],\n",
    "        template=prompt,\n",
    "    )\n",
    "\n",
    "    # Create the LangChain chain by piping the components together\n",
    "    summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "    # Invoke the chain with your repository data\n",
    "    return summary_chain.invoke({\n",
    "        \"description\": repo_description,\n",
    "        \"readme\": repo_readme\n",
    "    })\n",
    "\n",
    "# Print the final summary\n",
    "print(generated_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 1 prompts to test.\n"
     ]
    }
   ],
   "source": [
    "# prompts must include \"{description}\" and \"{readme}\"\n",
    "\n",
    "\n",
    "simple_prompt = \"\"\"\n",
    "        Please provide a summary of the following GitHub repository based on its description and README.md content.\n",
    "        If the README.md or description is not in English, please first translate it to English and then generate a summary.\n",
    "    \n",
    "        Repository description:\n",
    "        {description}\n",
    "    \n",
    "        README.md content:\n",
    "        {readme}\n",
    "    \n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "short_response_prompt = \"\"\"\n",
    "        Please provide a very short summary of the purpose following GitHub repository based on its description and README.md content.\n",
    "        If the README.md or description is not in English, please first translate it to English and then generate a summary.\n",
    "        Focus on the intent and purpose of the repository. Not the state or orginsation that created it, the programing language, or nature of the licence.\n",
    "\n",
    "        The summary should be very breif, around ten words, should not be sentances just relevant, expresive words.    \n",
    "        \n",
    "        Repository description:\n",
    "        {description}\n",
    "    \n",
    "        README.md content:\n",
    "        {readme}\n",
    "    \n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "bullets_prompt = \"\"\"\n",
    "        Please provide a summary bullets of the following GitHub repository based on its description and README.md content.\n",
    "        If the README.md or description is not in English, please first translate it to English and then generate a summary.\n",
    "        Focus on the intent and purpose of the repository. Not the state or orginsation that created it, the programing language, or nature of the licence.\n",
    "        Repository description:\n",
    "        {description}\n",
    "    \n",
    "        README.md content:\n",
    "        {readme}\n",
    "    \n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "perscriptive = \"\"\"\n",
    "        Please provide a summary of the following GitHub repository based on its description and README.md content.\n",
    "        These will be used for categorisation via embeddings.\n",
    "        If the README.md or description is not in English, please first translate it to English and then generate a summary.\n",
    "\n",
    "        The summary should be:\n",
    "            * concise and in fewer than 3 sentences.\n",
    "            * focus on what the repository is used for, enables, what the *intent* of it is\n",
    "        The summary should not:\n",
    "            * mention the country, state or organisation that the repository is for.\n",
    "            * include information about the type of license.\n",
    "            * comment on where more information can be found or what information was not available.\n",
    "            * mention the programing language used unless intrinsic to its purpose\n",
    "            * include the name of the repository unless its a standard word or discription needed to summarise\n",
    "    \n",
    "        Repository description:\n",
    "        {description}\n",
    "    \n",
    "        README.md content:\n",
    "        {readme}\n",
    "    \n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "prompts_to_test = {\n",
    "    # 'simple': simple_prompt ,\n",
    "    # 'bullets':bullets_prompt,\n",
    "    # 'perscriptive': perscriptive,\n",
    "    'short': short_response_prompt\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(prompts_to_test)} prompts to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate and Compare Summaries\n",
    "\n",
    "This next cell will iterate through each of the sampled repositories and generate a summary for each of the prompts you defined above. The results will be collected into a DataFrame for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing repository: rdl-standard...\n",
      "Processing repository: GeoNature-mobile-webapi...\n",
      "Processing repository: portal-brasil...\n",
      "Processing repository: Alouette_ISIS_extract...\n",
      "Processing repository: quadratic-voting-frontend...\n",
      "Processing repository: ala-install...\n",
      "Processing repository: gsoc-2023...\n",
      "Processing repository: volto-vlibras...\n",
      "Processing repository: ioos-code-sprint...\n",
      "Processing repository: ris-backend-service...\n",
      "Processing repository: openspace-android-sdk...\n",
      "Processing repository: steuerlotse...\n",
      "Processing repository: lexml-renderer-pdf...\n",
      "Processing repository: uswds...\n",
      "Processing repository: avh-hub...\n",
      "Processing repository: invitation-manager...\n",
      "Processing repository: ioos-python-package-skeleton...\n",
      "Processing repository: medlink...\n",
      "Processing repository: peacetrack-readme...\n",
      "Processing repository: restaurant-inspections...\n",
      "\n",
      "Finished processing all repositories.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for index, row in sample_df.iterrows():\n",
    "    print(f\"Processing repository: {row['name']}...\")\n",
    "    repo_info = {\n",
    "        'name': row['name'],\n",
    "        'description': row['description'],\n",
    "        'original_summary': row['summary']\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for prompt_name, prompt_text in prompts_to_test.items():\n",
    "        # Generate the summary using the placeholder function\n",
    "        # In a real run, this will call your LLM\n",
    "        summary = get_summary(prompt_text, repo_info['description'], row['readme'] )\n",
    "        repo_info[prompt_name] = summary\n",
    "        \n",
    "    results.append(repo_info)\n",
    "\n",
    "print(\"\\nFinished processing all repositories.\")\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Review the Results\n",
    "\n",
    "The table below shows the original summary alongside the new summaries generated by each of your prompts. This should make it easy to compare their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>original_summary</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rdl-standard</td>\n",
       "      <td>The Risk Data Library Standard (RDLS) is an open data standard to make it easier to work with disaster and climate risk data. It provides a common description of the data used and produced in risk assessments, including hazard, exposure, vulnerability, and modelled loss, or impact, data.</td>\n",
       "      <td>The Risk Data Library Standard (RDLS) is an open data standard facilitating work with disaster and climate risk data.  It provides a common framework for describing hazard, exposure, vulnerability, and loss data used in risk assessments.  The standard's development is coordinated through this repository, which publishes specifications and supports collaborative improvements.  The repository also encourages feedback and discussion on the evolving standard.</td>\n",
       "      <td>Standardizing disaster risk data: hazard, exposure, vulnerability, loss.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GeoNature-mobile-webapi</td>\n",
       "      <td>WebAPI (cot√© serveur) de synchronisation des donn√©es produites par GeoNature-mobile</td>\n",
       "      <td>This repository provides a web API for synchronizing data generated by the GeoNature-mobile application.  It enables importing data from GeoNature-mobile into a GeoNature PostgreSQL database. Synchronization can occur via network connection or by connecting a mobile device to a computer.  The API is designed to work with a separate synchronization application for PC-based syncing.</td>\n",
       "      <td>Data synchronization, GeoNature mobile, PostgreSQL database.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portal-brasil</td>\n",
       "      <td>Ambiente de desenvolvimento do PortalBrasil</td>\n",
       "      <td>This repository provides the development environment for the PortalBrasil website.  It enables the installation and running of both backend and frontend servers using provided make commands.  The repository includes instructions for managing backend and frontend packages.  Developers can easily set up and manage the PortalBrasil website using this environment.</td>\n",
       "      <td>PortalBrasil development environment, backend, frontend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alouette_ISIS_extract</td>\n",
       "      <td>üõ∞Ô∏è Ce code sert √† extraire les donn√©es et les m√©tadonn√©es des ionogrammes num√©ris√©s des satellites Alouette et ISIS |  üõ∞Ô∏è This code is an effort to extract data and metadata from the scanned ionogram images from the Alouette and ISIS satellites.</td>\n",
       "      <td>This repository provides tools and data for extracting information from scanned ionograms of the Alouette and ISIS satellites.  It offers a centralized repository of data and metadata from these satellites, enabling researchers to access and utilize this historical information for further research.  The repository includes open-source code for data processing, raw images, and a user-friendly application for data selection, download, and visualization.  The data covers over 60 years of ionospheric research.</td>\n",
       "      <td>Satellite ionogram data extraction,  research access,  data repository.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quadratic-voting-frontend</td>\n",
       "      <td>No description</td>\n",
       "      <td>This repository provides the frontend code for a quadratic voting system developed for the 2019 Taiwan Presidential Hackathon.  It includes sample data files (Proposal.json, ProposalPolls.json, User.json, UserAction.json) representing proposals, voting results, users, and user actions.  The repository does not contain backend code.  The data can be used for research and analysis.</td>\n",
       "      <td>Quadratic voting, frontend,  hackathon project,  data,  public use.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ala-install</td>\n",
       "      <td>Ansible playbooks for installing the ALA components</td>\n",
       "      <td>This repository provides Ansible playbooks for installing ALA components on Ubuntu 16 and later systems.  It includes a playbook for setting up an ALA demo. The playbooks are designed to facilitate the deployment of a Living Atlas, and integrate with supporting tools to simplify the process.  These tools assist in generating necessary configuration files and managing the Living Atlas portal.</td>\n",
       "      <td>ALA component installation, Ansible playbooks, Ubuntu deployment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gsoc-2023</td>\n",
       "      <td>Google Summer of Code 2023 with the Mayor's Office of New Urban Mechanics: Guidance + Ideas</td>\n",
       "      <td>This repository contains guidance and ideas developed during Google Summer of Code 2023 in collaboration with the Mayor's Office of New Urban Mechanics.  The project aims to provide support and resources.  The exact nature of the guidance and ideas is unspecified due to the lack of a README.</td>\n",
       "      <td>Google Summer of Code project: urban mechanics guidance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>volto-vlibras</td>\n",
       "      <td>An addon integrating the VLibras service into a Plone site running Volto</td>\n",
       "      <td>This repository provides a Volto add-on that integrates the VLibras service, enabling sign language interpretation within Plone websites.  It allows developers to easily add VLibras functionality to their Volto-based Plone projects.  Installation instructions and configuration details are included. The add-on is designed for use with Volto 18 and utilizes pnpm for package management.</td>\n",
       "      <td>Plone Volto addon: VLibras video interpretation integration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ioos-code-sprint</td>\n",
       "      <td>Information about IOOS Code Sprint activities.</td>\n",
       "      <td>This repository organizes the biannual IOOS Code Sprint, a four-day hackathon focused on addressing ocean data and information challenges.  The sprint brings together developers, researchers, and community members to work on projects supporting NOAA's Integrated Ocean Observing System mission.  Past sprints have been held both in-person and virtually.  Project ideas can be submitted via issues on this repository.</td>\n",
       "      <td>Ocean data, hackathon, collaboration, projects, information challenges.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ris-backend-service</td>\n",
       "      <td>RIS Caselaw</td>\n",
       "      <td>This repository, RIS Caselaw, provides a backend service.  It requires several CLI tools including Docker, Node.js, and Java.  The setup uses a container runtime and manages dependencies. The project includes tools for vulnerability scanning and managing architecture decision records.</td>\n",
       "      <td>Caselaw data, backend service,  legal information,  RIS system.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>openspace-android-sdk</td>\n",
       "      <td>Ordnance Survey OpenSpace Android SDK, an alternative to Google Maps API enabling access to Ordnance Survey data for the UK</td>\n",
       "      <td>The Ordnance Survey OpenSpace Android SDK provides access to Ordnance Survey map data for Android apps, offering a Google Maps API alternative.  It features map layers, gazetteer lookups, zoom/pan controls, annotations, and offline tile storage.  The SDK uses OSGB36 projection and supports user location display.  It enables fast, smooth map rendering with street-level detail for UK mapping.</td>\n",
       "      <td>UK mapping, Android SDK, Ordnance Survey data, Google Maps alternative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>steuerlotse</td>\n",
       "      <td>This is the code repository of the Steuerlotse by DigitalService.</td>\n",
       "      <td>This repository contains the discontinued source code for Steuerlotse, a web application enabling taxable pensioners in Germany to file their tax returns online.  Steuerlotse was designed specifically for pensioners without additional income.  The project originated from a simplified paper tax form and was further developed as a digital prototype.  The code is no longer actively developed but is maintained.</td>\n",
       "      <td>Online tax filing, pensioners, simplified returns, discontinued project.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lexml-renderer-pdf</td>\n",
       "      <td>Renderer Lexml para PDF</td>\n",
       "      <td>This repository provides a Lexml to PDF renderer.  It enables the conversion of Lexml files into PDF documents.  The project facilitates the rendering process, transforming Lexml data into a printable PDF format.</td>\n",
       "      <td>PDF rendering, Lexml input.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>uswds</td>\n",
       "      <td>The U.S. Web Design System helps the federal government build fast, accessible, mobile-friendly websites.</td>\n",
       "      <td>The United States Web Design System (USWDS) provides an open-source library of UI components and a visual style guide to help federal government websites be fast, accessible, and mobile-friendly.  It offers CSS and JavaScript components for building websites.  The system includes comprehensive documentation and a style guide.  USWDS aims to streamline the development of consistent and user-friendly government websites.</td>\n",
       "      <td>Federal government website, UI components, style guide, open source.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>avh-hub</td>\n",
       "      <td>Australian Virtual Herbarium</td>\n",
       "      <td>The Australian Virtual Herbarium Hub (avh-hub) is a Grails application providing a user interface and customizations to the ALA Biocache.  It deploys to Nexus via Travis-CI and can be deployed locally using Vagrant and Ansible or to an AWS EC2 production server using the same Ansible scripts.  The repository includes instructions for both local and production deployments.  The application uses Java 11 or 17 depending on the version.</td>\n",
       "      <td>Australian Virtual Herbarium:  web interface, Biocache integration, deployment scripts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>invitation-manager</td>\n",
       "      <td>The Invitation Manager is a tool that when installed on your website will allow the ability to create and manage invitation pop-ups on your website. | Le Gestionnaire d‚Äôinvitation est un outil qui, une fois install√© sur votre site Web, vous permettra de cr√©er et de g√©rer des invitations contextuelles sur votre site Web.</td>\n",
       "      <td>The Invitation Manager is a website tool enabling the creation and management of invitation pop-ups.  It provides installation and publishing manuals to guide users through the process.  The tool allows for easy deployment of invitation pop-ups on websites.  An example of the pop-ups is available.</td>\n",
       "      <td>Website, pop-up, invitations, creation, management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ioos-python-package-skeleton</td>\n",
       "      <td>Boilerplate repository for  IOOS packages</td>\n",
       "      <td>This repository provides a boilerplate for creating IOOS Python packages.  It offers a standardized structure and includes installation instructions using conda or pip.  The package includes example code demonstrating its basic functionality.  The project aims to streamline the development of new IOOS packages.</td>\n",
       "      <td>IOOS package template, standardized structure, simplified development.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>medlink</td>\n",
       "      <td>A project to make it easier for Peace Corps Volunteers to order their medical supplies from a local Peace Corps Office. Volunteers can order by sms or webform. PC Staff interact with the orders in a simple to use dashboard.</td>\n",
       "      <td>PC Medlink streamlines medical supply ordering for Peace Corps volunteers.  Volunteers can submit orders via SMS or a web form.  Peace Corps staff manage orders through a user-friendly dashboard.  The system was developed partly through a National Day of Civic Hacking and is available online.</td>\n",
       "      <td>Streamlined medical supply ordering: SMS, webform, dashboard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>peacetrack-readme</td>\n",
       "      <td>Creating a mobile solution for Volunteers to log their activity information in the field - Master Repo for documenting requirements</td>\n",
       "      <td>This repository documents the requirements for a mobile application designed to improve Peace Corps' monitoring and evaluation (M&amp;E) process.  The app will enable volunteers to easily log their field activities, facilitating data-driven decision-making within the agency.  It aims to increase volunteer reporting and ultimately transform Peace Corps' M&amp;E system.  The application will serve as a model for future M&amp;E tools.</td>\n",
       "      <td>Mobile volunteer activity logging; improved M&amp;E; data-driven decisions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>restaurant-inspections</td>\n",
       "      <td>Search restaurant inspections open data</td>\n",
       "      <td>This repository provides a searchable interface to Detroit restaurant inspection data.  It uses Gatsby, React, and GraphQL to query data from a PostgreSQL database, offering a user-friendly way to search for restaurants by name. The data is sourced from Detroit's open data portal and includes establishment details, inspection records, and violation information.  The repository provides instructions for setting up the development environment and deploying the application.</td>\n",
       "      <td>Detroit restaurant inspections, searchable database, open data.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set display options to show full text content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Display the DataFrame as an HTML table for better readability\n",
    "display(HTML(results_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering for Repository Summarization\n",
    "\n",
    "This notebook is designed to help you experiment with different prompts to generate high-quality summaries for government repositories. \n",
    "\n",
    "You can:\n",
    "1.  Load your repository data from a CSV file.\n",
    "2.  Take a random sample of repositories to test.\n",
    "3.  Define and test multiple summarization prompts.\n",
    "4.  Generate summaries using a language model.\n",
    "5.  Compare the results in a clear, side-by-side format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_FILE_PATH = '/home/theo/projects/iai/data/tmprunid/classified_gov_repositories_batch.csv'\n",
    "SAMPLE_SIZE = 10  # Number of random repositories to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Sample Data\n",
    "\n",
    "We'll load the data from the CSV file and select a random sample of repositories to work with. This keeps the process fast and manageable for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 500 repositories and sampled 20 of them.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>readme</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>rdl-standard</td>\n",
       "      <td># Risk Data Library Standard\\n\\nThe Risk Data Library Standard is a data model for describing Hazard, Exposure, \\nVulnerability and Loss data.\\n\\nThe model describes the common core metadata that applies to all risk datasets, \\nas well as standardised metadata that applies to Hazard, Exposure, Vulnerability and Loss \\ndata.\\n\\nThis repository is used to coordinate the development of this data model. It will \\nbe used to:\\n\\n* publish working and released drafts of the data model specifications\\n* coordinate collaboration and discussion around the iterative development of those specifications\\n* provide an overview of the current status and roadmap\\n\\n## Intended audience\\n\\nThe repository is intended to support the work of those developing and contributing to the \\nRisk Data Library specifications.\\n\\nThis repository is intended to:\\n\\n* support comments or feedback on the current specifications\\n* propose and discuss changes, e.g. in the form of revised wording or additions to the model\\n* answer questions about the governance and evolution of the standard\\n\\nOther more useful resources exist if you have general questions about the scope and goals \\nof [the Risk Data Library project](http://riskdatalibrary.org/), or are looking for a more [high-level introduction to \\nthe standard and its key concepts](https://docs.riskdatalibrary.org/).\\n\\n## How to contribute\\n\\nThe [Contributors guide](CONTRIBUTING.md) covers the different ways in which you can contribute to this project to \\nsupport the development and adoption of the Risk Data Library Standard.\\n\\n## Project governance\\n\\nRead [the project governance documentation](GOVERNANCE.md) for more detail about our approach to making decisions and \\nagreeing changes to the standard.\\n\\n## Licence\\n\\nThe published specifications and all working documents in this repository are published under \\na [Creative Commons Attribution-ShareAlike 4.0 (CC-BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode) licence.\\n\\nVisit the Creative Commons website for [official translations of the licence text](https://creativecommons.org/licenses/by-sa/4.0/legalcode#languages).\\n</td>\n",
       "      <td>The Risk Data Library Standard (RDLS) is an open data standard to make it easier to work with disaster and climate risk data. It provides a common description of the data used and produced in risk assessments, including hazard, exposure, vulnerability, and modelled loss, or impact, data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>GeoNature-mobile-webapi</td>\n",
       "      <td># GeoNature-mobile-webapi\\n\\nGeoNature est une application de saisie et de synth√®se des observations faune et flore : https://github.com/PnEcrins/GeoNature\\n\\nPour pouvoir importer les donn√©es saisies avec [Geonature-mobile](https://github.com/PnEcrins/GeoNature-mobile) dans la BDD PostgreSQL de GeoNature, cette web-API doit √™tre install√©e sur le serveur.\\n\\nLa synchronisation de ces donn√©es peut √™tre faite par le r√©seau (wifi ou 3G) ou en connectant le mobile en USB √† un PC connect√© √† internet. Dans ce cas, une application de synchronisation des donn√©es soit √™tre install√©e sur le PC : https://github.com/PnEcrins/GeoNature-mobile-sync \\n\\n![GeoNature schema general](https://github.com/PnEcrins/GeoNature/raw/master/docs/images/schema-geonature-environnement.jpg)\\n\\n## License\\n&amp;copy; Makina Corpus / Parc national des Ecrins 2012 - 2017\\n</td>\n",
       "      <td>WebAPI (cot√© serveur) de synchronisation des donn√©es produites par GeoNature-mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>portal-brasil</td>\n",
       "      <td>&lt;div align=\"center\"&gt;&lt;img alt=\"logo\" src=\"https://raw.githubusercontent.com/plonegovbr/plonegovbr.portal/main/docs/logo.png\" width=\"150\" /&gt;&lt;/div&gt;\\n\\n&lt;h1 align=\"center\"&gt;PortalBrasil&lt;/h1&gt;\\n\\nProjeto de desenvolvimento do Portal Brasil\\n\\n## Instala√ß√£o\\n\\nClone este reposit√≥rio\\n\\n```bash\\ngit clone git@github.com:plonegovbr/portal-brasil.git\\n```\\n\\nInstale as depend√™ncias de backend\\n\\n```bash\\nmake install-backend\\n```\\n\\nInstale as depend√™ncias de frontend\\n\\n```bash\\nmake install-frontend\\n```\\n\\n## Inicie os servidores\\n\\nInicie o servidor de backend\\n\\n```bash\\nmake start-backend\\n```\\nEm outro terminal, inicie o servidor de frontend:\\n\\n```bash\\nmake start-frontend\\n```\\n\\n## Pacotes em desenvolvimento\\n\\n### Backend\\n\\nEdite o arquivo `backend/mx.ini` e adicione / edite os pacotes e rode `make install-backend` novamente.\\n\\n### Frontend\\n\\nEdite o arquivo `frontend/mrs.developer.json` e adicione / edite os pacotes e rode `make install-frontend` novamente.\\n</td>\n",
       "      <td>Ambiente de desenvolvimento do PortalBrasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Alouette_ISIS_extract</td>\n",
       "      <td>\\n# Alouette-1, ISIS - 1 and ISIS -2 - Ionogram Data Extraction - Data from Canada's First Satellites Over 60 Years In the Making\\n\\n&gt; In this project, the film rolls from the Alouette and ISIS satellites were scanned, digitized, and made accessible to the public. The primary aim was to establish a centralized data repository, facilitating access for researchers to utilize both the data and metadata derived from the satellites for future research.\\n\\nAlouette -1 was the first topside ionospheric satellite and the first Canadian satellite launched in 1962 in collaboration with the United States through NASA. Alouette ‚Äì 1 was known for its swept frequency topside sounder experiment with the goal to investigate the geographic and diurnal variation of the topside ionosphere at altitudes up to 1000 km. One of the most important scientific results from Alouette-1 was that it provided the first global picture of electron-density distribution in the topside ionosphere. With the success of Alouette -1, Canada and the United States formally agreed on December 23rd, 1963, to extend their collaboration to a program called International Satellites for Ionospheric studies (ISIS). As part of this program, Canada designed and built an additional family of ionospheric satellites: Alouette ‚Äì 2, ISIS -1 and ISIS ‚Äì 2. The ISIS - 1 and ISIS - 2 satellites had a more complex navigational systems and larger data collection capabilities than Alouette- 1 and 2 satellites. For instance, ISIS ‚Äì 1 was the first in the series to contain a swept and fixed frequency sounder technique combined with a complete set of direct measurements.\\n\\nThe output from the topside sounders were a video signal that contained the ionospheric echo pulses, but also pulses that depicted frequency markers and when a new frame started. A system was built to read the 7-track reel-to-reel magnetic tapes displayed on a cathode ray tube in ‚ÄòB-scan‚Äô form. This product was called an ionogram, which depicted the reflections of radio waves emitted from the satellite off the top side of the ionosphere, across a range of frequencies. The scanning of the ionograms as the first step of the historical data restoration of the Alouette and ISIS satellites began in 2017. The processing of the Alouette \\n and ISIS data was concluded in 2023 and 2024 respectively. \\n\\n&gt; Canadian Space Agency has created a centralized repository, facilitating easy access for researchers to utilize both the data and metadata derived from the Alouette and ISIS satellites. This includes but is not limited to open-source code on the processing of the data, raw images, data dictionaries, detailed methodology and a micro application that provides users the ability to select, download and visualize Alouette and ISIS data.\\n\\n\\n## How to Get Started\\n**To learn how to access, work and re-process the data, read:**\\n\\n- [**Alouette-1 ‚Äì Ionogram Data Extraction Methodology**](https://github.com/asc-csa/Alouette_extract/blob/working/documentation/Alouette-1%20-</td>\n",
       "      <td>üõ∞Ô∏è Ce code sert √† extraire les donn√©es et les m√©tadonn√©es des ionogrammes num√©ris√©s des satellites Alouette et ISIS |  üõ∞Ô∏è This code is an effort to extract data and metadata from the scanned ionogram images from the Alouette and ISIS satellites.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>quadratic-voting-frontend</td>\n",
       "      <td># Quadratic Voting Frontend\\n\\nÊ≠§ËôïÁÇ∫[2019Á∏ΩÁµ±ÁõÉÈªëÂÆ¢Êùæ](https://presidential-hackathon.taiwan.gov.tw/)Âπ≥ÊñπÊäïÁ•®Ê≥ï‰πãÂâçÁ´ØÁ®ãÂºèÁ¢ºÔºå‰æõÂ§ßÁúæÂèÉËÄÉÂà©Áî®„ÄÇ\\n\\nÊ≥®ÊÑèÔºöÊ≠§Á®ãÂºèÁ¢º‰∏çÂê´ÂæåÁ´ØÁ®ãÂºèÁ¢º\\n\\nThis is the frontend code of [Taiwan Presidential Hackathon 2019](https://presidential-hackathon.taiwan.gov.tw/en/Default.aspx) quadratic voting page, and open under MIT License for public use.\\n\\nNotice: This code did not include backend.\\n\\n## ÊäïÁ•®ÁµêÊûú\\n\\nÊ≠§ËôïË≥áÊñôÁÇ∫ÊäïÁ•®Âæå‰πãË≥áÊñôÔºå‰æõÂ§ßÁúæÁ†îÁ©∂Âà©Áî®„ÄÇ\\n\\n### ÊèêÊ°àË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[Proposal.json](data/Proposal.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- ServiceAgencies: ÂúòÈ´î/Ê©üÊßãÂêçÁ®±\\n- TeamName: ÈöäÂêç\\n- ProposalTitle: È°åÁõÆ\\n\\n### ÊäïÁ•®ÁµêÊûúË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[ProposalPolls.json](data/ProposalPolls.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- Count: ÂæóÁ•®Êï∏\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n### ‰ΩøÁî®ËÄÖË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[User.json](data/User.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n### ‰ΩøÁî®ËÄÖÁ¥ÄÈåÑ\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[UserAction.json](data/UserAction.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- ActionID: Â∫èËôü\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- Sequence: Ë©≤Ê¨°ÊäïÁ•®Èö®Ê©üÊéíÂ∫èÂá∫ÁèæÂú®Á¨¨N‰Ωç\\n- Action: ÊäïÁ•®ÊàñÂõûÊî∂\\n  - Add: ÊäïÁ•®\\n  - Sub: Êî∂Âõû‰∏ÄÁ•®\\n- VoteCount: ÊäïÁ•®ÊàñÊî∂ÂõûÂæåÂâ©NÁ•®\\n- SessionID: SessionË≠òÂà•Á¢º\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n# Author\\n\\nÈô≥‰∏ñÁ••\\n\\n# License\\n\\n[MIT](License)\\n\\n</td>\n",
       "      <td>No description</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  \\\n",
       "83                rdl-standard   \n",
       "235    GeoNature-mobile-webapi   \n",
       "207              portal-brasil   \n",
       "168      Alouette_ISIS_extract   \n",
       "135  quadratic-voting-frontend   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       readme  \\\n",
       "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             # Risk Data Library Standard\\n\\nThe Risk Data Library Standard is a data model for describing Hazard, Exposure, \\nVulnerability and Loss data.\\n\\nThe model describes the common core metadata that applies to all risk datasets, \\nas well as standardised metadata that applies to Hazard, Exposure, Vulnerability and Loss \\ndata.\\n\\nThis repository is used to coordinate the development of this data model. It will \\nbe used to:\\n\\n* publish working and released drafts of the data model specifications\\n* coordinate collaboration and discussion around the iterative development of those specifications\\n* provide an overview of the current status and roadmap\\n\\n## Intended audience\\n\\nThe repository is intended to support the work of those developing and contributing to the \\nRisk Data Library specifications.\\n\\nThis repository is intended to:\\n\\n* support comments or feedback on the current specifications\\n* propose and discuss changes, e.g. in the form of revised wording or additions to the model\\n* answer questions about the governance and evolution of the standard\\n\\nOther more useful resources exist if you have general questions about the scope and goals \\nof [the Risk Data Library project](http://riskdatalibrary.org/), or are looking for a more [high-level introduction to \\nthe standard and its key concepts](https://docs.riskdatalibrary.org/).\\n\\n## How to contribute\\n\\nThe [Contributors guide](CONTRIBUTING.md) covers the different ways in which you can contribute to this project to \\nsupport the development and adoption of the Risk Data Library Standard.\\n\\n## Project governance\\n\\nRead [the project governance documentation](GOVERNANCE.md) for more detail about our approach to making decisions and \\nagreeing changes to the standard.\\n\\n## Licence\\n\\nThe published specifications and all working documents in this repository are published under \\na [Creative Commons Attribution-ShareAlike 4.0 (CC-BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode) licence.\\n\\nVisit the Creative Commons website for [official translations of the licence text](https://creativecommons.org/licenses/by-sa/4.0/legalcode#languages).\\n   \n",
       "235                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          # GeoNature-mobile-webapi\\n\\nGeoNature est une application de saisie et de synth√®se des observations faune et flore : https://github.com/PnEcrins/GeoNature\\n\\nPour pouvoir importer les donn√©es saisies avec [Geonature-mobile](https://github.com/PnEcrins/GeoNature-mobile) dans la BDD PostgreSQL de GeoNature, cette web-API doit √™tre install√©e sur le serveur.\\n\\nLa synchronisation de ces donn√©es peut √™tre faite par le r√©seau (wifi ou 3G) ou en connectant le mobile en USB √† un PC connect√© √† internet. Dans ce cas, une application de synchronisation des donn√©es soit √™tre install√©e sur le PC : https://github.com/PnEcrins/GeoNature-mobile-sync \\n\\n![GeoNature schema general](https://github.com/PnEcrins/GeoNature/raw/master/docs/images/schema-geonature-environnement.jpg)\\n\\n## License\\n&copy; Makina Corpus / Parc national des Ecrins 2012 - 2017\\n   \n",
       "207                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <div align=\"center\"><img alt=\"logo\" src=\"https://raw.githubusercontent.com/plonegovbr/plonegovbr.portal/main/docs/logo.png\" width=\"150\" /></div>\\n\\n<h1 align=\"center\">PortalBrasil</h1>\\n\\nProjeto de desenvolvimento do Portal Brasil\\n\\n## Instala√ß√£o\\n\\nClone este reposit√≥rio\\n\\n```bash\\ngit clone git@github.com:plonegovbr/portal-brasil.git\\n```\\n\\nInstale as depend√™ncias de backend\\n\\n```bash\\nmake install-backend\\n```\\n\\nInstale as depend√™ncias de frontend\\n\\n```bash\\nmake install-frontend\\n```\\n\\n## Inicie os servidores\\n\\nInicie o servidor de backend\\n\\n```bash\\nmake start-backend\\n```\\nEm outro terminal, inicie o servidor de frontend:\\n\\n```bash\\nmake start-frontend\\n```\\n\\n## Pacotes em desenvolvimento\\n\\n### Backend\\n\\nEdite o arquivo `backend/mx.ini` e adicione / edite os pacotes e rode `make install-backend` novamente.\\n\\n### Frontend\\n\\nEdite o arquivo `frontend/mrs.developer.json` e adicione / edite os pacotes e rode `make install-frontend` novamente.\\n   \n",
       "168  \\n# Alouette-1, ISIS - 1 and ISIS -2 - Ionogram Data Extraction - Data from Canada's First Satellites Over 60 Years In the Making\\n\\n> In this project, the film rolls from the Alouette and ISIS satellites were scanned, digitized, and made accessible to the public. The primary aim was to establish a centralized data repository, facilitating access for researchers to utilize both the data and metadata derived from the satellites for future research.\\n\\nAlouette -1 was the first topside ionospheric satellite and the first Canadian satellite launched in 1962 in collaboration with the United States through NASA. Alouette ‚Äì 1 was known for its swept frequency topside sounder experiment with the goal to investigate the geographic and diurnal variation of the topside ionosphere at altitudes up to 1000 km. One of the most important scientific results from Alouette-1 was that it provided the first global picture of electron-density distribution in the topside ionosphere. With the success of Alouette -1, Canada and the United States formally agreed on December 23rd, 1963, to extend their collaboration to a program called International Satellites for Ionospheric studies (ISIS). As part of this program, Canada designed and built an additional family of ionospheric satellites: Alouette ‚Äì 2, ISIS -1 and ISIS ‚Äì 2. The ISIS - 1 and ISIS - 2 satellites had a more complex navigational systems and larger data collection capabilities than Alouette- 1 and 2 satellites. For instance, ISIS ‚Äì 1 was the first in the series to contain a swept and fixed frequency sounder technique combined with a complete set of direct measurements.\\n\\nThe output from the topside sounders were a video signal that contained the ionospheric echo pulses, but also pulses that depicted frequency markers and when a new frame started. A system was built to read the 7-track reel-to-reel magnetic tapes displayed on a cathode ray tube in ‚ÄòB-scan‚Äô form. This product was called an ionogram, which depicted the reflections of radio waves emitted from the satellite off the top side of the ionosphere, across a range of frequencies. The scanning of the ionograms as the first step of the historical data restoration of the Alouette and ISIS satellites began in 2017. The processing of the Alouette \\n and ISIS data was concluded in 2023 and 2024 respectively. \\n\\n> Canadian Space Agency has created a centralized repository, facilitating easy access for researchers to utilize both the data and metadata derived from the Alouette and ISIS satellites. This includes but is not limited to open-source code on the processing of the data, raw images, data dictionaries, detailed methodology and a micro application that provides users the ability to select, download and visualize Alouette and ISIS data.\\n\\n\\n## How to Get Started\\n**To learn how to access, work and re-process the data, read:**\\n\\n- [**Alouette-1 ‚Äì Ionogram Data Extraction Methodology**](https://github.com/asc-csa/Alouette_extract/blob/working/documentation/Alouette-1%20-   \n",
       "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               # Quadratic Voting Frontend\\n\\nÊ≠§ËôïÁÇ∫[2019Á∏ΩÁµ±ÁõÉÈªëÂÆ¢Êùæ](https://presidential-hackathon.taiwan.gov.tw/)Âπ≥ÊñπÊäïÁ•®Ê≥ï‰πãÂâçÁ´ØÁ®ãÂºèÁ¢ºÔºå‰æõÂ§ßÁúæÂèÉËÄÉÂà©Áî®„ÄÇ\\n\\nÊ≥®ÊÑèÔºöÊ≠§Á®ãÂºèÁ¢º‰∏çÂê´ÂæåÁ´ØÁ®ãÂºèÁ¢º\\n\\nThis is the frontend code of [Taiwan Presidential Hackathon 2019](https://presidential-hackathon.taiwan.gov.tw/en/Default.aspx) quadratic voting page, and open under MIT License for public use.\\n\\nNotice: This code did not include backend.\\n\\n## ÊäïÁ•®ÁµêÊûú\\n\\nÊ≠§ËôïË≥áÊñôÁÇ∫ÊäïÁ•®Âæå‰πãË≥áÊñôÔºå‰æõÂ§ßÁúæÁ†îÁ©∂Âà©Áî®„ÄÇ\\n\\n### ÊèêÊ°àË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[Proposal.json](data/Proposal.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- ServiceAgencies: ÂúòÈ´î/Ê©üÊßãÂêçÁ®±\\n- TeamName: ÈöäÂêç\\n- ProposalTitle: È°åÁõÆ\\n\\n### ÊäïÁ•®ÁµêÊûúË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[ProposalPolls.json](data/ProposalPolls.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- Count: ÂæóÁ•®Êï∏\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n### ‰ΩøÁî®ËÄÖË≥áÊñô\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[User.json](data/User.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n### ‰ΩøÁî®ËÄÖÁ¥ÄÈåÑ\\n\\n#### Ê™îÊ°àÂêçÔºö\\n\\n[UserAction.json](data/UserAction.json)\\n\\n#### Ê¨Ñ‰ΩçË™™ÊòéÔºö\\n\\n- ActionID: Â∫èËôü\\n- UserID: ‰ΩøÁî®ËÄÖÁ∑®Ëôü\\n- ProposalID: ÊèêÊ°àÁ∑®Ëôü\\n- Sequence: Ë©≤Ê¨°ÊäïÁ•®Èö®Ê©üÊéíÂ∫èÂá∫ÁèæÂú®Á¨¨N‰Ωç\\n- Action: ÊäïÁ•®ÊàñÂõûÊî∂\\n  - Add: ÊäïÁ•®\\n  - Sub: Êî∂Âõû‰∏ÄÁ•®\\n- VoteCount: ÊäïÁ•®ÊàñÊî∂ÂõûÂæåÂâ©NÁ•®\\n- SessionID: SessionË≠òÂà•Á¢º\\n- CreateDate: Âª∫Á´ãÊôÇÈñìÔºàÊôÇÂçÄÔºöUTC+8Ôºâ\\n\\n# Author\\n\\nÈô≥‰∏ñÁ••\\n\\n# License\\n\\n[MIT](License)\\n\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                          description  \n",
       "83   The Risk Data Library Standard (RDLS) is an open data standard to make it easier to work with disaster and climate risk data. It provides a common description of the data used and produced in risk assessments, including hazard, exposure, vulnerability, and modelled loss, or impact, data.  \n",
       "235                                                                                                                                                                                                               WebAPI (cot√© serveur) de synchronisation des donn√©es produites par GeoNature-mobile  \n",
       "207                                                                                                                                                                                                                                                       Ambiente de desenvolvimento do PortalBrasil  \n",
       "168                                             üõ∞Ô∏è Ce code sert √† extraire les donn√©es et les m√©tadonn√©es des ionogrammes num√©ris√©s des satellites Alouette et ISIS |  üõ∞Ô∏è This code is an effort to extract data and metadata from the scanned ionogram images from the Alouette and ISIS satellites.  \n",
       "135                                                                                                                                                                                                                                                                                    No description  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(CSV_FILE_PATH)\n",
    "    # Ensure the sample size is not larger than the number of rows in the dataframe\n",
    "    sample_size = min(SAMPLE_SIZE, len(df))\n",
    "    if len(df) > sample_size:\n",
    "        sample_df = df.sample(n=sample_size, random_state=62) # random_state for reproducibility\n",
    "    else:\n",
    "        sample_df = df\n",
    "    print(f\"Successfully loaded {len(df)} repositories and sampled {len(sample_df)} of them.\")\n",
    "    display(sample_df[['name', 'readme', 'description']].head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {CSV_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define LLM Utility\n",
    "\n",
    "This is where you'll integrate your language model. The function `get_summary` is a placeholder. **You should replace its content with the logic from your `llm_utils.py` file.**\n",
    "\n",
    "The function should accept a `prompt` and the `text_to_summarize` and return the generated summary as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Prompts for Engineering\n",
    "\n",
    "Here you can define all the different prompts you want to test. I've added a few examples to get you started, focusing on different aspects like tone, format, and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reportify is a Python library that simplifies data reporting.  It transforms JSON or dictionary data into aesthetically pleasing PDF or HTML reports.  The library offers customizable templates and integrates easily with pandas DataFrames.  Users can generate formatted reports from complex data structures.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install necessary libraries if you haven't already\n",
    "# !pip install langchain-google-genai langchain python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from your .env file\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "# Securely get the API key from the environment.\n",
    "# This prevents the key from being stored in the notebook's code or output.\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Make sure it's in your .env file.\")\n",
    "\n",
    "# Define the model name (as seen in llm_utils.py)\n",
    "LLM_MODEL_NAME = \"gemini-1.5-flash\"\n",
    "\n",
    "# Initialize the Google Generative AI model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=LLM_MODEL_NAME,\n",
    "    google_api_key=google_api_key,\n",
    ")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# You can replace these variables with your own data\n",
    "repo_description = \"A Python library for parsing and generating beautiful, human-readable reports from complex data structures.\"\n",
    "repo_readme = \"\"\"\n",
    "# Reportify v1.2\n",
    "\n",
    "Reportify is a tool designed to make data reporting simple. It takes JSON or dictionary data and outputs clean, formatted reports in PDF or HTML.\n",
    "\n",
    "## Features\n",
    "- Multiple output formats (PDF, HTML)\n",
    "- Customizable templates\n",
    "- Easy integration with pandas DataFrames\n",
    "\n",
    "## Getting Started\n",
    "`pip install reportify`\n",
    "\"\"\"\n",
    "\n",
    "def get_summary(prompt: str, repo_description: str, repo_readme:str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a summary for the given text using a specified prompt.\n",
    "    This is a placeholder and should be replaced with your actual LLM implementation.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt to use for the summarization task.\n",
    "        repo_description: ...\n",
    "        repo_readme: ...\n",
    "\n",
    "    Returns:\n",
    "        The generated summary as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # This is the prompt template copied directly from llm_utils.py\n",
    "    summary_prompt = PromptTemplate(\n",
    "        input_variables=[\"description\", \"readme\"],\n",
    "        template=prompt,\n",
    "    )\n",
    "\n",
    "    # Create the LangChain chain by piping the components together\n",
    "    summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "    # Invoke the chain with your repository data\n",
    "    return summary_chain.invoke({\n",
    "        \"description\": repo_description,\n",
    "        \"readme\": repo_readme\n",
    "    })\n",
    "\n",
    "# Print the final summary\n",
    "print(generated_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 3 prompts to test.\n"
     ]
    }
   ],
   "source": [
    "# prompts must include \"{description}\" and \"{readme}\"\n",
    "\n",
    "\n",
    "simple_prompt = \"\"\"\n",
    "        Please provide a summary of the following GitHub repository based on its description and README.md content.\n",
    "        If the README.md or description is not in English, please first translate it to English and then generate a summary.\n",
    "    \n",
    "        Repository description:\n",
    "        {description}\n",
    "    \n",
    "        README.md content:\n",
    "        {readme}\n",
    "    \n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "bullets_prompt = \"\"\"\n",
    "        Please provide a summary bullets of the following GitHub repository based on its description and README.md content.\n",
    "        If the README.md or description is not in English, please first translate it to English and then generate a summary.\n",
    "        Focus on the intent and purpose of the repository. Not the state or orginsation that created it, the programing language, or nature of the licence.\n",
    "        Repository description:\n",
    "        {description}\n",
    "    \n",
    "        README.md content:\n",
    "        {readme}\n",
    "    \n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "perscriptive = \"\"\"\n",
    "        Please provide a summary of the following GitHub repository based on its description and README.md content.\n",
    "        These will be used for categorisation via embeddings.\n",
    "        If the README.md or description is not in English, please first translate it to English and then generate a summary.\n",
    "\n",
    "        The summary should be:\n",
    "            * concise and in fewer than 3 sentences.\n",
    "            * focus on what the repository is used for, enables, what the *intent* of it is\n",
    "        The summary should not:\n",
    "            * mention the country, state or organisation that the repository is for.\n",
    "            * include information about the type of license.\n",
    "            * comment on where more information can be found or what information was not available.\n",
    "            * mention the programing language used unless intrinsic to its purpose\n",
    "            * include the name of the repository unless its a standard word or discription needed to summarise\n",
    "    \n",
    "        Repository description:\n",
    "        {description}\n",
    "    \n",
    "        README.md content:\n",
    "        {readme}\n",
    "    \n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "prompts_to_test = {\n",
    "    'simple': simple_prompt ,\n",
    "    'bullets':bullets_prompt,\n",
    "    'perscriptive': perscriptive\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(prompts_to_test)} prompts to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate and Compare Summaries\n",
    "\n",
    "This next cell will iterate through each of the sampled repositories and generate a summary for each of the prompts you defined above. The results will be collected into a DataFrame for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing repository: rdl-standard...\n",
      "Processing repository: GeoNature-mobile-webapi...\n",
      "Processing repository: portal-brasil...\n",
      "Processing repository: Alouette_ISIS_extract...\n",
      "Processing repository: quadratic-voting-frontend...\n",
      "Processing repository: ala-install...\n",
      "Processing repository: gsoc-2023...\n",
      "Processing repository: volto-vlibras...\n",
      "Processing repository: ioos-code-sprint...\n",
      "Processing repository: ris-backend-service...\n",
      "Processing repository: openspace-android-sdk...\n",
      "Processing repository: steuerlotse...\n",
      "Processing repository: lexml-renderer-pdf...\n",
      "Processing repository: uswds...\n",
      "Processing repository: avh-hub...\n",
      "Processing repository: invitation-manager...\n",
      "Processing repository: ioos-python-package-skeleton...\n",
      "Processing repository: medlink...\n",
      "Processing repository: peacetrack-readme...\n",
      "Processing repository: restaurant-inspections...\n",
      "\n",
      "Finished processing all repositories.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for index, row in sample_df.iterrows():\n",
    "    print(f\"Processing repository: {row['name']}...\")\n",
    "    repo_info = {\n",
    "        'name': row['name'],\n",
    "        'description': row['description'],\n",
    "        'original_summary': row['summary']\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for prompt_name, prompt_text in prompts_to_test.items():\n",
    "        # Generate the summary using the placeholder function\n",
    "        # In a real run, this will call your LLM\n",
    "        summary = get_summary(prompt_text, repo_info['description'], row['readme'] )\n",
    "        repo_info[prompt_name] = summary\n",
    "        \n",
    "    results.append(repo_info)\n",
    "\n",
    "print(\"\\nFinished processing all repositories.\")\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Review the Results\n",
    "\n",
    "The table below shows the original summary alongside the new summaries generated by each of your prompts. This should make it easy to compare their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>original_summary</th>\n",
       "      <th>simple</th>\n",
       "      <th>bullets</th>\n",
       "      <th>perscriptive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rdl-standard</td>\n",
       "      <td>The Risk Data Library Standard (RDLS) is an open data standard to make it easier to work with disaster and climate risk data. It provides a common description of the data used and produced in risk assessments, including hazard, exposure, vulnerability, and modelled loss, or impact, data.</td>\n",
       "      <td>The Risk Data Library Standard (RDLS) is an open data standard facilitating work with disaster and climate risk data.  It provides a common framework for describing hazard, exposure, vulnerability, and loss data used in risk assessments.  The standard's development is coordinated through this repository, which publishes specifications and supports collaborative improvements.  The repository also encourages feedback and discussion on the evolving standard.</td>\n",
       "      <td>This GitHub repository hosts the Risk Data Library Standard (RDLS), an open data standard designed to simplify working with disaster and climate risk data.  The RDLS provides a common framework for describing data used in risk assessments, encompassing hazard, exposure, vulnerability, and loss/impact data.  The repository serves as a central hub for developing and refining the RDLS data model, facilitating collaboration, discussion, and the publication of model specifications.  It welcomes contributions and feedback from those involved in developing and using the standard, with guidelines for contribution and governance clearly outlined.  The standard itself, along with related documents, is licensed under a Creative Commons Attribution-ShareAlike 4.0 license.  The repository itself focuses on the technical development of the standard; users seeking broader information about the project should consult external resources linked within the README.</td>\n",
       "      <td>* **Standardize disaster and climate risk data:** The Risk Data Library Standard (RDLS) aims to create a common framework for describing and working with data related to disaster and climate risk.\\n\\n* **Define core metadata:**  It provides a standardized metadata model encompassing hazard, exposure, vulnerability, and loss/impact data, facilitating easier data sharing and analysis.\\n\\n* **Facilitate collaborative development:** The repository serves as a central hub for developing, refining, and discussing the RDLS specifications.  It supports contributions, feedback, and collaborative decision-making regarding the standard's evolution.\\n\\n* **Provide resources and support:** It offers resources for those involved in developing and using the RDLS, including guidance on contributing and information on project governance.</td>\n",
       "      <td>This repository facilitates the collaborative development of a standard data model for disaster and climate risk data.  It aims to create a common framework for describing hazard, exposure, vulnerability, and loss data, improving interoperability and data sharing.  The project supports feedback, change proposals, and discussion to refine the standard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GeoNature-mobile-webapi</td>\n",
       "      <td>WebAPI (cot√© serveur) de synchronisation des donn√©es produites par GeoNature-mobile</td>\n",
       "      <td>This repository provides a web API for synchronizing data generated by the GeoNature-mobile application.  It enables importing data from GeoNature-mobile into a GeoNature PostgreSQL database. Synchronization can occur via network connection or by connecting a mobile device to a computer.  The API is designed to work with a separate synchronization application for PC-based syncing.</td>\n",
       "      <td>This GitHub repository, `GeoNature-mobile-webapi`, contains a web API (server-side) for synchronizing data generated by the GeoNature-mobile application.  GeoNature is a data entry and synthesis application for fauna and flora observations. This API is necessary to import data from GeoNature-mobile into GeoNature's PostgreSQL database. Synchronization can occur over a network (Wi-Fi or 3G) or by connecting a mobile device via USB to a PC with internet access, using a separate synchronization application (`GeoNature-mobile-sync`).  The repository is licensed under a copyright held by Makina Corpus and the Ecrins National Park (2012-2017).</td>\n",
       "      <td>* Provides a web API (server-side) for synchronizing data generated by the GeoNature-mobile application.\\n* Enables importing data from GeoNature-mobile into a GeoNature PostgreSQL database.\\n* Supports data synchronization via network connection (Wi-Fi or 3G) or USB connection to a PC with internet access (requiring a separate synchronization application).\\n* Is a crucial component for integrating data collected using the GeoNature-mobile application into the central GeoNature database.</td>\n",
       "      <td>This web API synchronizes flora and fauna observation data from a mobile application to a PostgreSQL database.  It enables data import either via network connection or USB connection to a computer running a synchronization application.  The intent is to facilitate data transfer from mobile devices to a central database.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portal-brasil</td>\n",
       "      <td>Ambiente de desenvolvimento do PortalBrasil</td>\n",
       "      <td>This repository provides the development environment for the PortalBrasil website.  It enables the installation and running of both backend and frontend servers using provided make commands.  The repository includes instructions for managing backend and frontend packages.  Developers can easily set up and manage the PortalBrasil website using this environment.</td>\n",
       "      <td>This GitHub repository, `plonegovbr/portal-brasil`, contains the development environment for the PortalBrasil website.  The README provides instructions for setting up and running the project.  This involves cloning the repository, installing backend and frontend dependencies using `make` commands, and then starting separate backend and frontend servers, also using `make` commands.  The instructions also detail how to add or edit packages for both the backend and frontend by modifying specific configuration files (`backend/mx.ini` and `frontend/mrs.developer.json`) and then re-running the respective installation commands.</td>\n",
       "      <td>* This repository contains the development environment for the PortalBrasil website.\\n* It provides instructions for cloning the repository, installing backend and frontend dependencies, and starting backend and frontend servers.\\n*  The project allows for adding and editing packages for both the backend and frontend through configuration files (`backend/mx.ini` and `frontend/mrs.developer.json`).</td>\n",
       "      <td>This repository provides the development environment for a web portal.  It facilitates the installation and running of both backend and frontend servers, enabling the development and deployment of portal features.  The intent is to support the creation and maintenance of the portal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alouette_ISIS_extract</td>\n",
       "      <td>üõ∞Ô∏è Ce code sert √† extraire les donn√©es et les m√©tadonn√©es des ionogrammes num√©ris√©s des satellites Alouette et ISIS |  üõ∞Ô∏è This code is an effort to extract data and metadata from the scanned ionogram images from the Alouette and ISIS satellites.</td>\n",
       "      <td>This repository provides tools and data for extracting information from scanned ionograms of the Alouette and ISIS satellites.  It offers a centralized repository of data and metadata from these satellites, enabling researchers to access and utilize this historical information for further research.  The repository includes open-source code for data processing, raw images, and a user-friendly application for data selection, download, and visualization.  The data covers over 60 years of ionospheric research.</td>\n",
       "      <td>This GitHub repository contains code and resources for extracting data and metadata from digitized ionogram images obtained from the Alouette and ISIS satellites.  These satellites, launched by Canada (with US collaboration for Alouette-1), were pioneering topside ionospheric sounders, providing crucial data on the Earth's ionosphere.  The project involved scanning and digitizing historical film rolls containing ionograms‚Äîvisual representations of radio wave reflections from the ionosphere.  The repository provides open-source code used in this data processing, along with the raw images, data dictionaries, detailed methodologies, and a user-friendly application for accessing and visualizing the Alouette and ISIS datasets.  The data processing was completed in 2023 (Alouette) and 2024 (ISIS), and the data is now centrally available through the Canadian Space Agency.  The README also details the history and scientific significance of the Alouette and ISIS missions.</td>\n",
       "      <td>* **Centralized Data Repository:** The project aims to create a readily accessible, centralized repository for ionogram data and metadata from the Alouette and ISIS satellites.\\n\\n* **Data Extraction from Scanned Ionograms:**  The core function is to extract data and metadata from digitized scans of ionogram images.\\n\\n* **Facilitating Research:** The repository's purpose is to make this historical data easily available to researchers for future scientific study of the ionosphere.\\n\\n* **Comprehensive Resources:**  The repository includes not only the extracted data but also supporting materials such as open-source code, raw images, data dictionaries, and methodologies.  A user-friendly application is also provided for data selection, download, and visualization.</td>\n",
       "      <td>This project provides code to extract data and metadata from digitized ionogram images of Alouette and ISIS satellites.  Its purpose is to create a centralized, publicly accessible repository of this historical satellite data for research use.  The repository includes processed data, raw images, and documentation to aid researchers in accessing and re-processing the data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quadratic-voting-frontend</td>\n",
       "      <td>No description</td>\n",
       "      <td>This repository provides the frontend code for a quadratic voting system developed for the 2019 Taiwan Presidential Hackathon.  It includes sample data files (Proposal.json, ProposalPolls.json, User.json, UserAction.json) representing proposals, voting results, users, and user actions.  The repository does not contain backend code.  The data can be used for research and analysis.</td>\n",
       "      <td>This GitHub repository contains the frontend code for a quadratic voting system developed for the 2019 Taiwan Presidential Hackathon.  The code is open-source under the MIT license.  Importantly, the repository only includes the frontend; the backend code is not included.  The repository also provides sample data files (Proposal.json, ProposalPolls.json, User.json, UserAction.json) containing proposal information, voting results, user data, and user action logs respectively.  These data files are intended for research and analysis purposes.  The data includes fields such as proposal IDs, team names, user IDs, vote counts, timestamps, and action details (add/subtract votes).</td>\n",
       "      <td>* This repository provides the frontend code for a quadratic voting system developed for the 2019 Taiwan Presidential Hackathon.\\n* It includes sample data files (Proposal.json, ProposalPolls.json, User.json, UserAction.json) representing proposals, voting results, user information, and user voting activity.  These data files are provided for research and public use.\\n* Importantly, the repository only contains the frontend code; the backend is not included.</td>\n",
       "      <td>This repository provides a frontend interface for a quadratic voting system.  It includes sample data from a past voting event,  allowing for analysis and research of the quadratic voting process.  The code does not include the backend implementation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ala-install</td>\n",
       "      <td>Ansible playbooks for installing the ALA components</td>\n",
       "      <td>This repository provides Ansible playbooks for installing ALA components on Ubuntu 16 and later systems.  It includes a playbook for setting up an ALA demo. The playbooks are designed to facilitate the deployment of a Living Atlas, and integrate with supporting tools to simplify the process.  These tools assist in generating necessary configuration files and managing the Living Atlas portal.</td>\n",
       "      <td>This GitHub repository provides Ansible playbooks for installing ALA (Atlas of Living Australia) components on Ubuntu 16.04 and later versions.  The repository includes a playbook for setting up an ALA demo.  It requires Ansible version 2.17.3 (core) and 10.3.0 (community), with installation instructions provided for both APT (Linux) and OSX.  The playbooks are primarily tested on Ubuntu 20.04, with limited testing on other distributions.  The README also points to supporting tools: the Living Atlases Toolkit, the Living Atlas Generator (now superseded by the Toolkit), and a Yeoman generator, which assist in setting up and maintaining a Living Atlas portal by generating Ansible inventory files and potentially wrapper scripts.</td>\n",
       "      <td>* Provides Ansible playbooks for installing ALA (Atlas of Living Australia) components.\\n* Primarily supports Ubuntu 20.04, with limited support for other distributions.\\n* Includes a playbook for setting up an ALA demo.\\n* Specifies Ansible version 2.17.3 (core) and 10.3.0 (community) as prerequisites.\\n* Offers supporting tools (Living Atlases Toolkit, Living Atlas Generator, and a Yeoman generator) to simplify the setup and maintenance of ALA instances.  These tools assist in generating Ansible inventories needed by the playbooks.</td>\n",
       "      <td>This project provides Ansible playbooks to automate the installation of ALA components, primarily on Ubuntu systems.  It includes a demo setup and integrates with supporting tools to simplify the deployment and maintenance of a Living Atlas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gsoc-2023</td>\n",
       "      <td>Google Summer of Code 2023 with the Mayor's Office of New Urban Mechanics: Guidance + Ideas</td>\n",
       "      <td>This repository contains guidance and ideas developed during Google Summer of Code 2023 in collaboration with the Mayor's Office of New Urban Mechanics.  The project aims to provide support and resources.  The exact nature of the guidance and ideas is unspecified due to the lack of a README.</td>\n",
       "      <td>This GitHub repository contains materials related to a Google Summer of Code 2023 project undertaken with the Mayor's Office of New Urban Mechanics.  Since a README is unavailable, the specific content and goals of the project remain unknown.  The repository likely holds resources, code, or documentation associated with the summer of code project.</td>\n",
       "      <td>* This GitHub repository contains guidance and ideas developed during the Google Summer of Code 2023 project with the Mayor's Office of New Urban Mechanics.\\n* The repository's purpose is to share resources related to this specific project.  The exact nature of the guidance and ideas is not specified due to the lack of a README.</td>\n",
       "      <td>This project documents guidance and ideas developed during Google Summer of Code 2023, focusing on urban mechanics.  The intent is to provide resources and support for urban planning initiatives.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>volto-vlibras</td>\n",
       "      <td>An addon integrating the VLibras service into a Plone site running Volto</td>\n",
       "      <td>This repository provides a Volto add-on that integrates the VLibras service, enabling sign language interpretation within Plone websites.  It allows developers to easily add VLibras functionality to their Volto-based Plone projects.  Installation instructions and configuration details are included. The add-on is designed for use with Volto 18 and utilizes pnpm for package management.</td>\n",
       "      <td>This GitHub repository, `@plonegovbr/volto-vlibras`, is a Plone add-on that integrates the VLibras service (likely a Brazilian sign language interpreter) into a Plone website using the Volto frontend framework.  The add-on provides a widget that can be easily added to a Volto site, enabling users to access real-time sign language interpretation.  The README provides installation instructions, which involve adding the add-on to a Volto project's `package.json` and configuring its placement within the application using `appExtras`.  Development of this add-on uses pnpm workspaces and is designed for Volto version 18, requiring pnpm as the package manager.  The repository includes unit tests, Storybook integration for component development, and a `make` system for streamlined development tasks.  Screenshots demonstrate the widget's functionality.</td>\n",
       "      <td>* This repository provides a Volto add-on that integrates the VLibras service, enabling sign language interpretation, into a Plone website.\\n* It offers a widget that activates VLibras video interpretation.\\n* Installation instructions are provided for adding the addon to existing Volto projects.\\n* Configuration details are given to integrate the VLibras widget into the website's interface.\\n* The repository includes development instructions, requiring Volto 18 and pnpm.  Development utilizes pnpm workspaces and `mrs-developer`.\\n*  Testing and build commands are provided.</td>\n",
       "      <td>This addon integrates the VLibras service into a Plone website using Volto.  It provides a widget enabling users to access sign language interpretation. The intent is to improve accessibility for deaf or hard-of-hearing users.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ioos-code-sprint</td>\n",
       "      <td>Information about IOOS Code Sprint activities.</td>\n",
       "      <td>This repository organizes the biannual IOOS Code Sprint, a four-day hackathon focused on addressing ocean data and information challenges.  The sprint brings together developers, researchers, and community members to work on projects supporting NOAA's Integrated Ocean Observing System mission.  Past sprints have been held both in-person and virtually.  Project ideas can be submitted via issues on this repository.</td>\n",
       "      <td>This GitHub repository serves as the central hub for organizing the bi-annual IOOS Code Sprint, a 4-day hackathon hosted by NOAA's Integrated Ocean Observing System Office.  The sprint brings together developers, researchers, and community members to work on projects addressing data and information challenges related to ocean, coastal, and Great Lakes observation.  The repository contains information about past and future sprints, including dates, locations (often hybrid in-person/virtual), and registration details.  Participants can contribute project ideas via issues on the repository.  The README specifically details information for the 2024 sprint (held in Washington, D.C.) and provides links to further details and registration.  Information about the 2022 sprint (held in Chicago) is also included, along with a link to a proposed track list.</td>\n",
       "      <td>* This GitHub repository serves as an organizational hub for the biannual IOOS Code Sprint.\\n\\n* The Code Sprint is a four-day hackathon focused on addressing data and information challenges related to ocean, coastal, and Great Lakes observations.\\n\\n*  Participants include developers, researchers, and community members working on projects aligned with the IOOS mission of providing high-quality ocean information for safety, economic, and stewardship purposes.\\n\\n* The repository facilitates idea contributions for the sprint via issue tracking.  Specific details about dates, locations (including virtual participation options), and further information are provided for each sprint iteration.</td>\n",
       "      <td>This repository organizes a biannual hackathon focused on ocean data and information challenges.  The event brings together developers and researchers to collaborate on projects improving ocean observation data.  It aims to address critical data issues and enhance the accessibility of ocean information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ris-backend-service</td>\n",
       "      <td>RIS Caselaw</td>\n",
       "      <td>This repository, RIS Caselaw, provides a backend service.  It requires several CLI tools including Docker, Node.js, and Java.  The setup uses a container runtime and manages dependencies. The project includes tools for vulnerability scanning and managing architecture decision records.</td>\n",
       "      <td>This GitHub repository, `ris-backend-service`, is the backend for a RIS (presumably, a Retrieval and Indexing System) Caselaw application.  The README details the prerequisites for setting up the development environment, which includes various CLI tools like Lefthook, Docker, Node.js, Java (version 21), and optional tools for JSON/YAML processing, linting, vulnerability scanning, and ADR management.  A `Brewfile` is provided for simplified installation using Homebrew.  The repository also uses GitHub Actions for pipeline and security scanning, and SonarCloud for code quality monitoring.  The README mentions the need for S3 credentials for lookup table initialization, implying interaction with cloud storage.  In short, it's a backend service for a legal caselaw application built using Java and Node.js, managed with various DevOps tools.</td>\n",
       "      <td>* This repository provides a backend service for RIS Caselaw.\\n* It utilizes various tools for development, including Docker for containerization, Node.js for JavaScript runtime, and Java for backend processes.\\n*  The setup requires installing several command-line tools for managing git hooks, accessing secrets, JSON and YAML processing, linting, vulnerability scanning, and managing Architecture Decision Records (ADRs).  A `brew bundle` command simplifies installation for Homebrew users.\\n*  The project uses S3 credentials for lookup table initialization (further details are cut off in the provided text).</td>\n",
       "      <td>This repository provides a backend service for case law.  It uses Java and requires several CLI tools for setup and operation, including Docker for containerization.  The intent is to facilitate access and management of legal case data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>openspace-android-sdk</td>\n",
       "      <td>Ordnance Survey OpenSpace Android SDK, an alternative to Google Maps API enabling access to Ordnance Survey data for the UK</td>\n",
       "      <td>The Ordnance Survey OpenSpace Android SDK provides access to Ordnance Survey map data for Android apps, offering a Google Maps API alternative.  It features map layers, gazetteer lookups, zoom/pan controls, annotations, and offline tile storage.  The SDK uses OSGB36 projection and supports user location display.  It enables fast, smooth map rendering with street-level detail for UK mapping.</td>\n",
       "      <td>The Ordnance Survey OpenSpace Android SDK is a native Android framework providing access to Ordnance Survey (OS) map data for UK applications.  It offers a Google Maps API v2-like interface for easy integration, supporting various map layers, gazetteer lookups, and features like annotations, overlays, and offline tile storage.  Key functionalities include zoom/pan controls, OSGB36 National Grid projection support (with WGS84 conversion), user location display, and fast rendering via OpenGL.  The SDK utilizes OS Web Map Tile Services (WMTS) and offers both online and offline capabilities, mirroring features found in OS MapFinder.  It's fully supported by Ordnance Survey, including ongoing updates and user support.  The repository provides the SDK framework and demo applications to aid developers in integrating OS mapping into their Android projects.</td>\n",
       "      <td>* Provides an Android SDK for accessing Ordnance Survey (OS) map data in the UK, offering an alternative to Google Maps API.\\n* Offers a familiar API similar to Google Maps API v2 for easy transition.\\n* Enables access to multiple OS mapping layers and gazetteer lookups.\\n* Supports features like zoom/pan controls, annotations, overlays, and offline tile storage.\\n* Includes a geocoder for searching places of interest using various OS datasets (online and offline).\\n* Uses OSGB36 British National Grid projection and handles conversions between WGS84 and OSGB36.\\n* Leverages OpenGL for efficient map rendering.\\n* Provides access to detailed street-level mapping and countryside/National Park data.\\n* Benefits include official OS support, online and offline capabilities, fast rendering, and smooth panning.\\n* Available as a static framework for easy integration into Android applications.</td>\n",
       "      <td>This Android SDK provides access to mapping data, offering features like zoom, pan, annotation, and offline tile storage.  It aims to simplify the integration of mapping data into Android applications, providing a similar API to a well-known alternative.  The SDK utilizes OpenGL for efficient map rendering and includes a geocoder for searching points of interest.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>steuerlotse</td>\n",
       "      <td>This is the code repository of the Steuerlotse by DigitalService.</td>\n",
       "      <td>This repository contains the discontinued source code for Steuerlotse, a web application enabling taxable pensioners in Germany to file their tax returns online.  Steuerlotse was designed specifically for pensioners without additional income.  The project originated from a simplified paper tax form and was further developed as a digital prototype.  The code is no longer actively developed but is maintained.</td>\n",
       "      <td>This GitHub repository contains the source code for Steuerlotse, a discontinued online tax return submission tool for pensioners in Germany without additional income.  Developed by DigitalService GmbH des Bundes as a digital prototype based on a simplified paper tax form, it allowed eligible pensioners to file their taxes online.  The project, initially a Tech4Germany Fellowship 2020 initiative, was shut down at the end of 2022, and while the code is still available under a license, it is no longer actively developed but only maintained.  Contributions are welcome, following the provided guidelines and code of conduct.</td>\n",
       "      <td>* **Purpose:** The Steuerlotse project aimed to provide a simplified online tax return system for pensioners in Germany without additional income.  It was based on a pre-existing paper form.\\n\\n* **Functionality:**  The online tool allowed eligible pensioners to submit their tax returns digitally.\\n\\n* **Status:** The project is discontinued and the code is no longer under active development.  Maintenance only is being performed.</td>\n",
       "      <td>This repository contains the discontinued source code for a web application that enabled pensioners without additional income to file their taxes online.  It was a digital prototype based on a simplified paper tax form.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lexml-renderer-pdf</td>\n",
       "      <td>Renderer Lexml para PDF</td>\n",
       "      <td>This repository provides a Lexml to PDF renderer.  It enables the conversion of Lexml files into PDF documents.  The project facilitates the rendering process, transforming Lexml data into a printable PDF format.</td>\n",
       "      <td>This GitHub repository, `lexml-renderer-pdf`, contains a PDF renderer for Lexml files.  The description and README are minimal, but the core functionality is clearly to convert Lexml documents into PDF format.</td>\n",
       "      <td>* This repository provides a PDF renderer for Lexml files.\\n* Its purpose is to convert Lexml data into a PDF format.</td>\n",
       "      <td>This repository provides a PDF renderer for Lexml files.  Its purpose is to convert Lexml data into a PDF format.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>uswds</td>\n",
       "      <td>The U.S. Web Design System helps the federal government build fast, accessible, mobile-friendly websites.</td>\n",
       "      <td>The United States Web Design System (USWDS) provides an open-source library of UI components and a visual style guide to help federal government websites be fast, accessible, and mobile-friendly.  It offers CSS and JavaScript components for building websites.  The system includes comprehensive documentation and a style guide.  USWDS aims to streamline the development of consistent and user-friendly government websites.</td>\n",
       "      <td>This GitHub repository, `uswds/uswds`, contains the source code for the United States Web Design System (USWDS).  USWDS is an open-source library of UI components and a visual style guide designed to help federal government websites be fast, accessible, and mobile-friendly.  The repository provides CSS, JavaScript, and Sass files for building websites.  Documentation and a live preview of the design system are available at designsystem.digital.gov, maintained in a separate repository.  This repository includes details on installation, usage, customization (including theming and Sass compilation), accessibility features, browser support, and contribution guidelines.  The README highlights the recent 3.0 release and provides links to release notes and a migration guide.</td>\n",
       "      <td>* Provides a library of open-source UI components and a visual style guide for U.S. federal government websites.\\n* Aims to help the federal government build fast, accessible, and mobile-friendly websites.\\n* Offers a design system codebase, with separate documentation and website maintained in a different repository.\\n* Includes CSS and JavaScript components,  supporting Sass compilation for customization and theming.\\n* Emphasizes accessibility and provides guidance on browser support.\\n* Maintains long-term support for its versions.\\n* Encourages community contributions.</td>\n",
       "      <td>This repository provides a web design system, offering open-source UI components and a visual style guide.  Its intent is to help build fast, accessible, and mobile-friendly websites.  The system includes CSS and JavaScript components for easy integration into projects.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>avh-hub</td>\n",
       "      <td>Australian Virtual Herbarium</td>\n",
       "      <td>The Australian Virtual Herbarium Hub (avh-hub) is a Grails application providing a user interface and customizations to the ALA Biocache.  It deploys to Nexus via Travis-CI and can be deployed locally using Vagrant and Ansible or to an AWS EC2 production server using the same Ansible scripts.  The repository includes instructions for both local and production deployments.  The application uses Java 11 or 17 depending on the version.</td>\n",
       "      <td>The GitHub repository `avh-hub` contains a Grails application serving as the user interface and customization layer for the Australian Virtual Herbarium (AVH).  It's built on top of the ALA Biocache (another Grails plugin).  The README details deployment procedures, including: deploying new versions to Nexus (using Travis-CI), deploying to a local virtual machine using Vagrant and Ansible, and deploying to an AWS EC2 production server, also using Ansible.  The repository supports two major versions: 8.x (Java 17) and 7.x (Java 11).  Deployment involves updating version numbers in configuration files and utilizing Ansible playbooks for automated installation and configuration on various environments.</td>\n",
       "      <td>* Provides a user interface and customizations for the Australian Virtual Herbarium (AVH) website.\\n* Integrates with the ALA Biocache system via a Grails plugin.\\n* Offers deployment instructions for various environments:  local virtual machines using Vagrant and Ansible, and AWS EC2 production servers, also using Ansible.\\n* Uses Travis-CI for automated deployment to Nexus.  Manual intervention is required to update version numbers in relevant configuration files.</td>\n",
       "      <td>This repository provides a user interface and customizations for a biological data hub.  It uses a Grails application and integrates with an existing biocache system for deployment to various environments including virtual machines and cloud servers.  The intent is to facilitate the management and access of biological data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>invitation-manager</td>\n",
       "      <td>The Invitation Manager is a tool that when installed on your website will allow the ability to create and manage invitation pop-ups on your website. | Le Gestionnaire d‚Äôinvitation est un outil qui, une fois install√© sur votre site Web, vous permettra de cr√©er et de g√©rer des invitations contextuelles sur votre site Web.</td>\n",
       "      <td>The Invitation Manager is a website tool enabling the creation and management of invitation pop-ups.  It provides installation and publishing manuals to guide users through the process.  The tool allows for easy deployment of invitation pop-ups on websites.  An example of the pop-ups is available.</td>\n",
       "      <td>The Invitation Manager (version 1.2.2) is a tool for creating and managing invitation pop-ups on websites.  The repository provides an example of the pop-up in action, along with installation and publishing guides (available in English and French).  Version history is also accessible via a link to the releases page.  The tool is designed to be integrated into websites to facilitate the display of invitation pop-ups.</td>\n",
       "      <td>* The Invitation Manager is a tool for creating and managing invitation pop-ups on websites.\\n* It provides a method for easily deploying and managing these pop-ups.\\n* Installation and publishing guides are available.\\n* A version history is maintained.</td>\n",
       "      <td>This tool enables the creation and management of invitation pop-ups on websites.  It provides a user-friendly interface for managing these pop-ups and includes installation and publishing guides. The intent is to simplify the process of deploying and managing website invitations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ioos-python-package-skeleton</td>\n",
       "      <td>Boilerplate repository for  IOOS packages</td>\n",
       "      <td>This repository provides a boilerplate for creating IOOS Python packages.  It offers a standardized structure and includes installation instructions using conda or pip.  The package includes example code demonstrating its basic functionality.  The project aims to streamline the development of new IOOS packages.</td>\n",
       "      <td>This GitHub repository, `ioos_pkg_skeleton`, provides a boilerplate for creating Python packages within the Integrated Ocean Observing System (IOOS) ecosystem.  The README details installation via `conda` or `pip`, shows a simple usage example, and includes links to documentation, issue tracking, and the license (BSD-3-Clause).  Essentially, it's a template to simplify the process of building new IOOS-related Python packages.</td>\n",
       "      <td>* Provides a boilerplate repository for creating packages.\\n* Offers a standardized structure and setup for new packages.\\n* Includes instructions for installation using conda and pip.\\n* Presents a simple example demonstrating basic package functionality.\\n* Encourages users to report bugs and suggest features via GitHub.</td>\n",
       "      <td>This repository provides a boilerplate for creating packages.  It offers a template and installation instructions using conda or pip.  The intent is to simplify the process of developing and deploying  packages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>medlink</td>\n",
       "      <td>A project to make it easier for Peace Corps Volunteers to order their medical supplies from a local Peace Corps Office. Volunteers can order by sms or webform. PC Staff interact with the orders in a simple to use dashboard.</td>\n",
       "      <td>PC Medlink streamlines medical supply ordering for Peace Corps volunteers.  Volunteers can submit orders via SMS or a web form.  Peace Corps staff manage orders through a user-friendly dashboard.  The system was developed partly through a National Day of Civic Hacking and is available online.</td>\n",
       "      <td>PC Medlink is a web application designed to streamline the process of Peace Corps volunteers ordering medical supplies.  Volunteers can place orders via SMS or a web form, and Peace Corps staff manage these orders through a user-friendly dashboard.  The project, featured on the White House Office of Science and Technology Blog, originated from a National Day of Civic Hacking and is available live at pcmedlink.org.  The README provides instructions for setting up a local development environment using Ruby on Rails, including database setup and administrative user creation.  The project utilizes continuous integration via Travis CI and encourages bug reporting via email to a designated support address.</td>\n",
       "      <td>* **Streamlines Medical Supply Ordering for Peace Corps Volunteers:** The project simplifies the process for Peace Corps volunteers to request medical supplies from their local Peace Corps office.\\n\\n* **Multiple Ordering Methods:** Volunteers can place orders via SMS text message or a web form.\\n\\n* **User-Friendly Dashboard for Staff:** Peace Corps staff manage orders through an easy-to-use dashboard.\\n\\n* **Open-Source and Collaborative:** The project is open-source, encouraging community contributions and improvements.  It includes instructions for local development and testing.</td>\n",
       "      <td>This project facilitates medical supply ordering for volunteers via SMS or web forms and provides staff with a dashboard to manage orders.  It aims to streamline the process of obtaining necessary medical supplies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>peacetrack-readme</td>\n",
       "      <td>Creating a mobile solution for Volunteers to log their activity information in the field - Master Repo for documenting requirements</td>\n",
       "      <td>This repository documents the requirements for a mobile application designed to improve Peace Corps' monitoring and evaluation (M&amp;E) process.  The app will enable volunteers to easily log their field activities, facilitating data-driven decision-making within the agency.  It aims to increase volunteer reporting and ultimately transform Peace Corps' M&amp;E system.  The application will serve as a model for future M&amp;E tools.</td>\n",
       "      <td>This GitHub repository documents the requirements for a mobile application designed to improve the Monitoring and Evaluation (M&amp;E) process for Peace Corps Volunteers.  The app aims to streamline how volunteers log their field activities, enabling better data collection and analysis for the agency.  The project's goal is to create a more efficient and effective M&amp;E system, leading to data-driven decision-making within Peace Corps and encouraging higher volunteer reporting rates.  The repository's primary content is a requirements document outlining the app's functionality and specifications.</td>\n",
       "      <td>* **Develop a mobile application:**  The primary goal is to create a mobile application for Peace Corps volunteers.\\n\\n* **Improve Monitoring and Evaluation (M&amp;E):** The app aims to enhance the current M&amp;E process used by Peace Corps to assess volunteer impact.\\n\\n* **Streamline data collection and reporting:** The application will improve the data structure and reporting workflow for volunteer activities.\\n\\n* **Drive data-driven decision-making:** The project intends to enable Peace Corps to make more informed decisions based on data collected through the app.\\n\\n* **Increase volunteer reporting:**  The app is expected to increase the number of volunteers who report their activities.\\n\\n* **Serve as a change agent:** The application is envisioned as a catalyst for positive change within the Peace Corps' M&amp;E system.</td>\n",
       "      <td>This repository documents the requirements for a mobile application designed to improve volunteer activity logging.  The app aims to modernize impact assessment and reporting, enabling data-driven decision-making within the organization.  It intends to serve as a catalyst for improved volunteer reporting and organizational efficiency.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>restaurant-inspections</td>\n",
       "      <td>Search restaurant inspections open data</td>\n",
       "      <td>This repository provides a searchable interface to Detroit restaurant inspection data.  It uses Gatsby, React, and GraphQL to query data from a PostgreSQL database, offering a user-friendly way to search for restaurants by name. The data is sourced from Detroit's open data portal and includes establishment details, inspection records, and violation information.  The repository provides instructions for setting up the development environment and deploying the application.</td>\n",
       "      <td>This GitHub repository, `restaurant-inspections`, is a Gatsby-based website that searches restaurant inspection data from the Detroit Health Department.  The data, sourced from a PostgreSQL database (a dump of which is included), is queried using Gatsby's `gatsby-source-pg` plugin and exposed through a GraphQL API.  The website allows users to search for restaurants by name.  The repository includes instructions for setting up the development environment (requiring PostgreSQL and the Gatsby CLI), deploying to GitHub Pages, and customizing the styling (using React Semantic UI with Less).  The data is also available on Detroit's open data portal.  The project leverages PostGraphile for efficient data querying and includes custom PostgreSQL functions to optimize search functionality.</td>\n",
       "      <td>* **Purpose:** To create a searchable website displaying restaurant inspection data from the Detroit Health Department.\\n\\n* **Data Source:**  The website utilizes a PostgreSQL database containing restaurant establishment, inspection, and violation data,  also available on Detroit's open data portal.\\n\\n* **Functionality:**  The site provides a search function allowing users to find restaurants based on name.  The search functionality leverages a custom PostgreSQL function and GraphQL queries for efficient data retrieval.\\n\\n* **Technology:**  Built using Gatsby (a React static site generator) and GraphQL, with data sourced from PostgreSQL.  The front-end uses React Semantic UI with Less for styling.</td>\n",
       "      <td>This project provides a searchable interface for restaurant inspection data.  It uses a GraphQL API built with Gatsby to query a PostgreSQL database, enabling efficient searching and retrieval of establishment details and inspection results.  The intent is to make this open data easily accessible and usable.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set display options to show full text content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Display the DataFrame as an HTML table for better readability\n",
    "display(HTML(results_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
